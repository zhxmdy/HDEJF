{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Price forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  \n",
    "import matplotlib.pyplot as plt  \n",
    "from VMD import VMD  \n",
    "alpha = 2000       # moderate bandwidth constraint  \n",
    "tau = 0.            # noise-tolerance (no strict fidelity enforcement)  \n",
    "K =2             # 3 modes  \n",
    "DC = 0             # no DC part imposed  \n",
    "init = 1           # initialize omegas uniformly  \n",
    "tol = 1e-7  \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.tsa.seasonal import STL\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.linear_model import( LinearRegression, \n",
    "                                 ElasticNet, \n",
    "                                 Ridge, \n",
    "                                 Lasso, \n",
    "                                 HuberRegressor)\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor,AdaBoostRegressor, BaggingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import numpy as np\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from arch import arch_model\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "gold_prices=pd.read_csv(\"gold.csv\") #load data\n",
    "varpre=pd.read_csv('shuju.csv') #REPRESENT VAR model prediction results\n",
    "gold_prices=gold_prices.iloc[:,0:]\n",
    "gold_prices['Period']=pd.to_datetime(gold_prices['Period'])\n",
    "gold_prices.set_index('Period',inplace=True)\n",
    "with pd.ExcelWriter('DEprice-proposed-mult_sheets.xlsx') as writer:\n",
    "    for r in range(10):  # \n",
    "      \n",
    "        def create_lag_features(data, lag):\n",
    "            lagged_data = pd.DataFrame(index=data.index)\n",
    "            for col in data.columns:\n",
    "                if col != 'Period':\n",
    "                    for k in range(1, lag + 1):\n",
    "                        lagged_data[f'{col}_Lag_{k}'] = data[col].shift(k)\n",
    "#                     lagged_data=lagged_data.drop(f'{col}_Lag_1',axis=1)\n",
    "            return lagged_data\n",
    "\n",
    "\n",
    "        lag = 3 # lag\n",
    "        # features = create_lag_features(gold_prices, lag)\n",
    "        feature = gold_prices\n",
    "\n",
    "        window_size = 44  # window size\n",
    "\n",
    "        # initionlzation\n",
    "        modelss = {\n",
    "            'TrendModel': BaggingRegressor(estimator=KNeighborsRegressor(n_neighbors=5), n_estimators=100),\n",
    "            'SeasonalityModel': BaggingRegressor(estimator=KNeighborsRegressor(n_neighbors=5), n_estimators=100),\n",
    "            'ResidualModel': BaggingRegressor(estimator=KNeighborsRegressor(n_neighbors=5), n_estimators=100),\n",
    "                'TrendModel3':RandomForestRegressor(),\n",
    "            'SeasonalityModel3': RandomForestRegressor(),\n",
    "            'ResidualModel3': RandomForestRegressor(),\n",
    "            'TrendModel4': LinearRegression(),\n",
    "            'SeasonalityModel4': LinearRegression(),\n",
    "            'ResidualModel4': LinearRegression()\n",
    "        #         'Lasso': Lasso(0.5, fit_intercept=False)\n",
    "        }\n",
    "\n",
    "        # train and predict\n",
    "        # predictions = {model_name: [] for model_name in models}\n",
    "        actual_price = []\n",
    "        direction_correct = 0\n",
    "        predictions=[]\n",
    "        # features=features.fillna(method='bfill')\n",
    "        # features=features.dropna()\n",
    "        # print(features)\n",
    "        for i in range(len(gold_prices['GoldPrice']) - window_size):\n",
    "\n",
    "            # STL decomposition\n",
    "            stl = STL(gold_prices['GoldPrice'].iloc[0:i + window_size])\n",
    "            result = stl.fit()\n",
    "            trend = result.trend\n",
    "            seasonalo = result.seasonal\n",
    "            residualo = result.resid\n",
    "            newdata=seasonalo+residualo\n",
    "            u, u_hat, omega = VMD(newdata, alpha, tau, K, DC, init, tol)\n",
    "            u=u.astype(float)\n",
    "            seasonal = pd.DataFrame(u[1],index=gold_prices.index[0:(i + window_size)])\n",
    "            seasonal.columns = ['seasonal']\n",
    "            residual = pd.DataFrame(u[0],index=gold_prices.index[0:(i + window_size)])\n",
    "            residual.columns = ['residual']\n",
    "\n",
    "\n",
    "            feature['Trend'] = trend\n",
    "            feature['Seasonal'] = seasonal\n",
    "            feature['Residual'] = residual\n",
    "            #create after features\n",
    "        #     features = create_lag_features(feature, lag)\n",
    "        #     names=['GoldPrice','TrendModel','SeasonalityModel','ResidualModel']\n",
    "        #     features[names] = feature[names]\n",
    "        #     print(feature.shape)\n",
    "\n",
    "        #    '''delete factors'''\n",
    "            featuretrend=feature.drop(['GoldPrice','GoldPricede','MAgold','diffgold','MAdemand','diffdemand','Seasonal','Residual'], axis=1)\n",
    "            featureseasonal=feature.drop(['Trend','Residual'], axis=1)\n",
    "            featureresidual=feature.drop(['GoldPrice','GoldPricede','MAgold','diffgold','MAdemand','diffdemand','Seasonal','Trend'], axis=1)\n",
    "            \n",
    "#             featuretrend=feature.drop(['GoldPricede','MAdemand','diffdemand','Seasonal','Residual'], axis=1)\n",
    "#             featureseasonal=feature.drop(['Trend','Residual'], axis=1)\n",
    "#             featureresidual=feature.drop(['GoldPricede','MAdemand','diffdemand','Seasonal','Trend'], axis=1)\n",
    "\n",
    "            modelt=ARIMA(trend[0:i + window_size], order=(0, 1, 1))\n",
    "            resultt = modelt.fit()\n",
    "            trend_pred1 = resultt.forecast()[0]\n",
    "\n",
    "            featuretrend = create_lag_features(featuretrend, lag)\n",
    "            featuretrend['Trend']=feature['Trend']\n",
    "        #     featuretrend = featuretrend.drop(featuretrend.head(lag).index)\n",
    "            xtraint=featuretrend.iloc[lag:i + window_size,0:-1]\n",
    "            ytraint=featuretrend.iloc[lag:i + window_size,-1]\n",
    "            xtestt=featuretrend.iloc[i+window_size,0:-1]\n",
    "        #     xtestt=np.array(xtestt).reshape(-1,featuretrend.shape[1]-1)\n",
    "\n",
    "            featureseasonal = create_lag_features(featureseasonal, lag)\n",
    "            featureseasonal['Seasonal']=feature['Seasonal']\n",
    "        #     featuretrend = featuretrend.drop(featuretrend.head(lag).index)\n",
    "            xtrains=featureseasonal.iloc[lag:i + window_size,0:-1]\n",
    "            ytrains=featureseasonal.iloc[lag:i + window_size,-1]\n",
    "            xtests=featureseasonal.iloc[i+window_size,0:-1]\n",
    "        #     xtests=np.array(xtests).reshape(-1,featureseasonal.shape[1]-1)\n",
    "\n",
    "            models=ARIMA(residual[0:i + window_size], order=(0, 1, 1))\n",
    "        #     models=SARIMAX(seasonal[0:i + window_size], order=(1, 1, 1), seasonal_order=(1, 1, 1, 4))\n",
    "            results = models.fit()\n",
    "            residual_pred1 = results.forecast()[0]\n",
    "\n",
    "            featureresidual = create_lag_features(featureresidual, lag)\n",
    "            featureresidual['Residual']=feature['Residual']\n",
    "        #     featuretrend = featuretrend.drop(featuretrend.head(lag).index)\n",
    "            xtrainr=featureresidual.iloc[lag:i + window_size,0:-1]\n",
    "            ytrainr=featureresidual.iloc[lag:i + window_size,-1]\n",
    "            xtestr=featureresidual.iloc[i+window_size,0:-1]\n",
    "        #     xtestr=np.array(xtestr).reshape(-1,featureresidual.shape[1]-1)\n",
    "\n",
    "        #     scaler = MinMaxScaler()\n",
    "        # #             scaler = StandardScaler()\n",
    "        #     X_train = scaler.fit_transform(X_train)\n",
    "        #     X_test = scaler.transform(X_test)\n",
    "        #     scaler1 = MinMaxScaler()\n",
    "        #     y_trend = scaler1.fit_transform(y_trend.values.reshape(-1, 1)).flatten()\n",
    "        #     y_seasonal = scaler1.transform(y_seasonal.values.reshape(-1, 1)).flatten()\n",
    "        #     y_residual = scaler1.transform(y_residual.values.reshape(-1, 1)).flatten()\n",
    "            modelr=ARIMA(seasonal[0:i + window_size], order=(0, 1, 1))\n",
    "        #     models=SARIMAX(seasonal[0:i + window_size], order=(1, 1, 1), seasonal_order=(1, 1, 1, 4))\n",
    "            resultr = modelr.fit()\n",
    "            seasonal_pred1 = resultr.forecast()[0]\n",
    "\n",
    "#             \n",
    "#             modelfs = RandomForestRegressor(random_state=10)\n",
    "#             \n",
    "#             modelfs.fit(xtraint,ytraint)\n",
    "#            \n",
    "#             importances = modelfs.feature_importances_\n",
    "#       \n",
    "#             threshold = np.median(importances)\n",
    "#             \n",
    "#             selected_features = xtraint.columns[importances > threshold]\n",
    "#             xtraint=xtraint[selected_features]\n",
    "#             xtestt=xtestt[selected_features]\n",
    "            xtestt=np.array(xtestt).reshape(-1,xtraint.shape[1])\n",
    "\n",
    "                \n",
    "#             modelfsr = RandomForestRegressor(random_state=10)\n",
    "#            \n",
    "#             modelfsr.fit(xtrainr,ytrainr)\n",
    "#             \n",
    "#             importancesr = modelfsr.feature_importances_\n",
    "#            \n",
    "#             thresholdr = np.median(importancesr)\n",
    "#       \n",
    "#             selected_featuresr = xtrainr.columns[importancesr > thresholdr]\n",
    "#             xtrainr=xtrainr[selected_featuresr]\n",
    "#             xtestr=xtestr[selected_featuresr]\n",
    "            xtestr=np.array(xtestr).reshape(-1,xtrainr.shape[1])  \n",
    "\n",
    "                    # create RF model\n",
    "#             modelfss = RandomForestRegressor(random_state=10)\n",
    "#             # \n",
    "#             modelfss.fit(xtrains,ytrains)\n",
    "#             # \n",
    "#             importancess = modelfss.feature_importances_\n",
    "#             # \n",
    "#             thresholds = np.median(importancess)\n",
    "#           \n",
    "#             selected_featuress = xtrains.columns[importancess > thresholds]\n",
    "#             xtrains=xtrains[selected_featuress]\n",
    "#             xtests=xtests[selected_featuress]\n",
    "            xtests=np.array(xtests).reshape(-1,xtrains.shape[1])       \n",
    "\n",
    "\n",
    "            modelss['SeasonalityModel'].fit(xtrains, ytrains)\n",
    "            modelss['TrendModel'].fit(xtraint, ytraint)\n",
    "            modelss['ResidualModel'].fit(xtrainr, ytrainr)\n",
    "\n",
    "            # forecasting\n",
    "\n",
    "            trend_pred2 = modelss['TrendModel'].predict(xtestt)\n",
    "            seasonal_pred2 = modelss['SeasonalityModel'].predict(xtests)\n",
    "            residual_pred2 = modelss['ResidualModel'].predict(xtestr)\n",
    "\n",
    "            modelss['SeasonalityModel3'].fit(xtrains, ytrains)\n",
    "            modelss['TrendModel3'].fit(xtraint, ytraint)\n",
    "            modelss['ResidualModel3'].fit(xtrainr, ytrainr)\n",
    "\n",
    "#             modelss['SeasonalityModel4'].fit(xtrains, ytrains)\n",
    "#             modelss['TrendModel4'].fit(xtraint, ytraint)\n",
    "#             modelss['ResidualModel4'].fit(xtrainr, ytrainr)\n",
    "            # 进行预测\n",
    "\n",
    "            trend_pred3 = modelss['TrendModel3'].predict(xtestt)\n",
    "            seasonal_pred3 = modelss['SeasonalityModel3'].predict(xtests)\n",
    "            residual_pred3 = modelss['ResidualModel3'].predict(xtestr)\n",
    "            \n",
    "#             trend_pred4 = modelss['TrendModel4'].predict(xtestt)\n",
    "#             seasonal_pred4 = modelss['SeasonalityModel4'].predict(xtests)\n",
    "#             residual_pred4 = modelss['ResidualModel4'].predict(xtestr)\n",
    "\n",
    "            #inverse normalzation\n",
    "        #     trend_pred = scaler1.inverse_transform(np.array(trend_pred).reshape(-1, 1)).flatten()\n",
    "        #     seasonal_pred = scaler1.inverse_transform(np.array(seasonal_pred).reshape(-1, 1)).flatten()\n",
    "        #     residual_pred = scaler1.inverse_transform(np.array(residual_pred).reshape(-1, 1)).flatten()\n",
    "\n",
    "          \n",
    "#             predicted = trend_pred2+ seasonal_pred1 + residual_pred2\n",
    "            predicted = (trend_pred2+trend_pred3)/2 +seasonal_pred1 + (residual_pred2+residual_pred3)/2\n",
    "\n",
    "#             predicted = (trend_pred2+ seasonal_pred1 + residual_pred2+trend_pred1+ seasonal_pred1 + residual_pred1+trend_pred3+ seasonal_pred1 + residual_pred3)/3\n",
    "            actual_price.append(gold_prices['GoldPrice'].iloc[i + window_size])\n",
    "            predicted=0.5*predicted+0.5*varpre.iloc[i,1]   # average weighting ensemble no dynamic weighting ensemble\n",
    "       \n",
    "            if i < len(feature) - window_size:\n",
    "                    actual_change = gold_prices['GoldPrice'].iloc[i + window_size ] - gold_prices['GoldPrice'].iloc[i + window_size -1]\n",
    "                    predicted_change = predicted - gold_prices['GoldPrice'].iloc[i + window_size -1]\n",
    "                    if np.sign(actual_change) == np.sign(predicted_change):\n",
    "                        direction_correct += 1\n",
    "\n",
    "            predictions.append(predicted)\n",
    "        predictions = np.concatenate([arr for arr in predictions])\n",
    "        actual_price = np.array(actual_price)\n",
    "        # coputing metrics\n",
    "        mse = mean_squared_error(actual_price, predictions)\n",
    "        rmse = np.sqrt(mse)\n",
    "        mae = mean_absolute_error(actual_price, predictions)\n",
    "        mape = np.mean(np.abs((actual_price - predictions) / actual_price)) * 100\n",
    "        total_directions = len(feature) - window_size \n",
    "  \n",
    "        direction_accuracy = (direction_correct/ total_directions) * 100\n",
    "\n",
    "        metrics = {\n",
    "                'RMSE': rmse,\n",
    "                'MSE': mse,\n",
    "                'MAE': mae,\n",
    "                'MAPE':mape,\n",
    "                'Directional Accuracy': direction_accuracy,\n",
    "                'actual_price':actual_price,\n",
    "                'predictions':predictions\n",
    "            }\n",
    "        print(metrics)\n",
    "\n",
    "\n",
    "        # df_metrics = pd.DataFrame(metrics)\n",
    "        # metrics.to_excel('model_metrics.xlsx', index=metrics)\n",
    "        print(actual_price)\n",
    "        print(predictions)\n",
    "        df_metrics = pd.DataFrame(metrics)\n",
    "        # df_metrics.to_excel('model_metrics_random_forest84.xlsx', index=metrics)\n",
    "        df_metrics.to_excel(writer, sheet_name=f'Run_{r}', index=metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demand forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  \n",
    "import matplotlib.pyplot as plt  \n",
    "from VMD import VMD  \n",
    "#. some sample parameters for VMD  \n",
    "alpha = 2000       # moderate bandwidth constraint  \n",
    "tau = 0.            # noise-tolerance (no strict fidelity enforcement)  \n",
    "K =3             # 3 modes  \n",
    "DC = 0             # no DC part imposed  \n",
    "init = 1           # initialize omegas uniformly  \n",
    "tol = 1e-7  \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.tsa.seasonal import STL\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.linear_model import( LinearRegression, \n",
    "                                 ElasticNet, \n",
    "                                 Ridge, \n",
    "                                 Lasso, \n",
    "                                 HuberRegressor)\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor,AdaBoostRegressor, BaggingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import numpy as np\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from arch import arch_model\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "gold_prices=pd.read_csv(\"gold.csv\")\n",
    "varpre=pd.read_csv('shuju.csv')\n",
    "gold_prices=gold_prices.iloc[:,0:]\n",
    "gold_prices['Period']=pd.to_datetime(gold_prices['Period'])\n",
    "gold_prices.set_index('Period',inplace=True)\n",
    "with pd.ExcelWriter('DEdemand-Proposed-baggingarimamult_sheets.xlsx') as writer:\n",
    "    for r in range(10):  # \n",
    "\n",
    "        def create_lag_features(data, lag):\n",
    "            lagged_data = pd.DataFrame(index=data.index)\n",
    "            for col in data.columns:\n",
    "                if col != 'Period':\n",
    "                    for k in range(1, lag + 1):\n",
    "                        lagged_data[f'{col}_Lag_{k}'] = data[col].shift(k)\n",
    "        #             lagged_data=lagged_data.drop(f'{col}_Lag_1',axis=1)\n",
    "            return lagged_data\n",
    "\n",
    "        lag = 3 # set lag\n",
    "        # features = create_lag_features(gold_prices, lag)\n",
    "        feature = gold_prices\n",
    "\n",
    "        window_size = 44  # window size\n",
    "\n",
    "        # initlazation model\n",
    "        modelss = {\n",
    "            'TrendModel': BaggingRegressor(estimator=KNeighborsRegressor(n_neighbors=5), n_estimators=100),\n",
    "            'SeasonalityModel': BaggingRegressor(estimator=KNeighborsRegressor(n_neighbors=5), n_estimators=100),\n",
    "            'ResidualModel': BaggingRegressor(estimator=KNeighborsRegressor(n_neighbors=5), n_estimators=100),\n",
    "            'lasso1':RandomForestRegressor(),\n",
    "            'lasso2': RandomForestRegressor(),\n",
    "            'lasso3':RandomForestRegressor()\n",
    "        }\n",
    "\n",
    "        #train and predict\n",
    "        # predictions = {model_name: [] for model_name in models}\n",
    "        actual_price = []\n",
    "        direction_correct = 0\n",
    "        predictions=[]\n",
    "        # features=features.fillna(method='bfill')\n",
    "        # features=features.dropna()\n",
    "        # print(features)\n",
    "        for i in range(len(gold_prices['GoldPricede']) - window_size):\n",
    "\n",
    "            # STL\n",
    "            u, u_hat, omega = VMD(gold_prices['GoldPricede'].iloc[0:i + window_size], alpha, tau, K, DC, init, tol)\n",
    "            u=u.astype(float)\n",
    "            trend = pd.DataFrame(u[0],index=gold_prices.index[0:(i + window_size)])\n",
    "            trend.columns = ['trend']\n",
    "\n",
    "            newdata = pd.DataFrame(gold_prices['GoldPricede'].iloc[0:i + window_size]-u[0],index=gold_prices.index[0:(i + window_size)])\n",
    "            stl = STL(newdata)\n",
    "            result = stl.fit()\n",
    "            seasonal = result.seasonal+result.resid\n",
    "            residual = result.trend\n",
    "\n",
    "\n",
    "            feature['Trend'] = trend\n",
    "            feature['Seasonal'] = seasonal\n",
    "            feature['Residual'] = residual\n",
    "            #create after features\n",
    "        #     features = create_lag_features(feature, lag)\n",
    "        #     names=['GoldPrice','TrendModel','SeasonalityModel','ResidualModel']\n",
    "        #     features[names] = feature[names]\n",
    "        #     print(feature.shape)\n",
    "\n",
    "        #     print(features.shape)\n",
    "            featuretrend=feature.drop(['GoldPrice','GoldPricede','MAgold','diffgold','MAdemand','diffdemand','Seasonal','Residual'], axis=1)\n",
    "            featureseasonal=feature.drop(['GoldPricede','MAdemand','diffdemand','Trend','Residual'], axis=1)\n",
    "            featureresidual=feature.drop(['GoldPricede','MAdemand','diffdemand','Seasonal','Trend'], axis=1)\n",
    "            #'''no external factors results'''\n",
    "#             featuretrend=feature.drop(['GoldPrice','GoldPricede','MAgold','diffgold','MAdemand','diffdemand','Seasonal','Residual'], axis=1)\n",
    "#             featureseasonal=feature.drop(['GoldPrice','GoldPricede','MAgold','diffgold','MAdemand','diffdemand','Trend','Residual'], axis=1)\n",
    "#             featureresidual=feature.drop(['GoldPrice','GoldPricede','MAgold','diffgold','MAdemand','diffdemand','Seasonal','Trend'], axis=1)\n",
    "\n",
    "            modelt=ARIMA(trend[0:i + window_size], order=(0, 1, 1))\n",
    "            resultt = modelt.fit()\n",
    "            trend_pred1 = resultt.forecast()[0]\n",
    "\n",
    "            featuretrend = create_lag_features(featuretrend, lag)\n",
    "            featuretrend['Trend']=feature['Trend']\n",
    "        #     featuretrend = featuretrend.drop(featuretrend.head(lag).index)\n",
    "            xtraint=featuretrend.iloc[lag:i + window_size,0:-1]\n",
    "            ytraint=featuretrend.iloc[lag:i + window_size,-1]\n",
    "            xtestt=featuretrend.iloc[i+window_size,0:-1]\n",
    "            xtestt=np.array(xtestt).reshape(-1,featuretrend.shape[1]-1)\n",
    "\n",
    "            featureseasonal = create_lag_features(featureseasonal, lag)\n",
    "            featureseasonal['Seasonal']=feature['Seasonal']\n",
    "        #     featuretrend = featuretrend.drop(featuretrend.head(lag).index)\n",
    "            xtrains=featureseasonal.iloc[lag:i + window_size,0:-1]\n",
    "            ytrains=featureseasonal.iloc[lag:i + window_size,-1]\n",
    "            xtests=featureseasonal.iloc[i+window_size,0:-1]\n",
    "        #     xtests=np.array(xtests).reshape(-1,featureseasonal.shape[1]-1)\n",
    "\n",
    "            models=ARIMA(seasonal[0:i + window_size], order=(0, 1, 1))\n",
    "        #     models=SARIMAX(seasonal[0:i + window_size], order=(1, 1, 1), seasonal_order=(1, 1, 1, 4))\n",
    "            results = models.fit()\n",
    "            seasonal_pred1 = results.forecast()[0]\n",
    "\n",
    "            featureresidual = create_lag_features(featureresidual, lag)\n",
    "            featureresidual['Residual']=feature['Residual']\n",
    "        #     featuretrend = featuretrend.drop(featuretrend.head(lag).index)\n",
    "            xtrainr=featureresidual.iloc[lag:i + window_size,0:-1]\n",
    "            ytrainr=featureresidual.iloc[lag:i + window_size,-1]\n",
    "            xtestr=featureresidual.iloc[i+window_size,0:-1]\n",
    "\n",
    "\n",
    "            models=ARIMA(residual[0:i + window_size], order=(0, 1, 1))\n",
    "        #     models=SARIMAX(seasonal[0:i + window_size], order=(1, 1, 1), seasonal_order=(1, 1, 1, 4))\n",
    "            results = models.fit()\n",
    "            residual_pred1 = results.forecast()[0]\n",
    "\n",
    "        #     xtestr=np.array(xtestr).reshape(-1,featureresidual.shape[1]-1)\n",
    "\n",
    "        #     scaler = MinMaxScaler()\n",
    "        # #             scaler = StandardScaler()\n",
    "        #     X_train = scaler.fit_transform(X_train)\n",
    "        #     X_test = scaler.transform(X_test)\n",
    "        #     scaler1 = MinMaxScaler()\n",
    "        #     y_trend = scaler1.fit_transform(y_trend.values.reshape(-1, 1)).flatten()\n",
    "        #     y_seasonal = scaler1.transform(y_seasonal.values.reshape(-1, 1)).flatten()\n",
    "        #     y_residual = scaler1.transform(y_residual.values.reshape(-1, 1)).flatten()\n",
    "\n",
    "           \n",
    "            modelfs = RandomForestRegressor(random_state=0)\n",
    "          \n",
    "            modelfs.fit(xtrains,ytrains)\n",
    "           \n",
    "            importances = modelfs.feature_importances_\n",
    "            \n",
    "            threshold = np.median(importances)\n",
    "            \n",
    "            selected_features = xtrains.columns[importances > threshold]\n",
    "            xtrains=xtrains[selected_features]\n",
    "            xtests=xtests[selected_features]\n",
    "            xtests=np.array(xtests).reshape(-1,xtrains.shape[1])\n",
    "\n",
    "     \n",
    "            modelfsr = RandomForestRegressor(random_state=0)\n",
    "     \n",
    "            modelfsr.fit(xtrainr,ytrainr)\n",
    "\n",
    "            importancesr = modelfsr.feature_importances_\n",
    "          \n",
    "            thresholdr = np.median(importances)\n",
    "        \n",
    "            selected_featuresr = xtrainr.columns[importancesr > thresholdr]\n",
    "            xtrainr=xtrainr[selected_featuresr]\n",
    "            xtestr=xtestr[selected_featuresr]\n",
    "            xtestr=np.array(xtestr).reshape(-1,xtrainr.shape[1])  \n",
    "\n",
    "            modelss['SeasonalityModel'].fit(xtrains, ytrains)\n",
    "            modelss['TrendModel'].fit(xtraint, ytraint)\n",
    "            modelss['ResidualModel'].fit(xtrainr, ytrainr)\n",
    "\n",
    "            modelss['lasso2'].fit(xtrains, ytrains)\n",
    "            modelss['lasso1'].fit(xtraint, ytraint)\n",
    "            modelss['lasso3'].fit(xtrainr, ytrainr)\n",
    "  \n",
    "\n",
    "            trend_pred2 = modelss['TrendModel'].predict(xtestt)\n",
    "            seasonal_pred2 = modelss['SeasonalityModel'].predict(xtests)\n",
    "            residual_pred2 = modelss['ResidualModel'].predict(xtestr)\n",
    "            \n",
    "            trend_pred3 = modelss['lasso1'].predict(xtestt)\n",
    "            seasonal_pred3 = modelss['lasso2'].predict(xtests)\n",
    "            residual_pred3 = modelss['lasso3'].predict(xtestr)\n",
    "\n",
    "\n",
    "        #     trend_pred = scaler1.inverse_transform(np.array(trend_pred).reshape(-1, 1)).flatten()\n",
    "        #     seasonal_pred = scaler1.inverse_transform(np.array(seasonal_pred).reshape(-1, 1)).flatten()\n",
    "        #     residual_pred = scaler1.inverse_transform(np.array(residual_pred).reshape(-1, 1)).flatten()\n",
    "\n",
    "            predicted = trend_pred1+ seasonal_pred2 + residual_pred2\n",
    "#             predicted = (trend_pred1+ seasonal_pred2 + residual_pred2+trend_pred1+ seasonal_pred3 + residual_pred3)/2\n",
    "\n",
    "        \n",
    "            actual_price.append(gold_prices['GoldPricede'].iloc[i + window_size])\n",
    "            predicted=0.9*predicted+0.1*varpre.iloc[i,2]    \n",
    "       \n",
    "            if i < len(feature) - window_size:\n",
    "                    actual_change = gold_prices['GoldPricede'].iloc[i + window_size ] - gold_prices['GoldPricede'].iloc[i + window_size -1]\n",
    "                    predicted_change = predicted - gold_prices['GoldPricede'].iloc[i + window_size -1]\n",
    "                    if np.sign(actual_change) == np.sign(predicted_change):\n",
    "                        direction_correct += 1\n",
    "\n",
    "            predictions.append(predicted)\n",
    "        predictions = np.concatenate([arr for arr in predictions])\n",
    "        actual_price = np.array(actual_price)\n",
    "        # computing metrics\n",
    "        mse = mean_squared_error(actual_price, predictions)\n",
    "        rmse = np.sqrt(mse)\n",
    "        mae = mean_absolute_error(actual_price, predictions)\n",
    "        mape = np.mean(np.abs((actual_price - predictions) / actual_price)) * 100\n",
    "        total_directions = len(feature) - window_size \n",
    "      \n",
    "        direction_accuracy = (direction_correct/ total_directions) * 100\n",
    "\n",
    "        metrics = {\n",
    "                'RMSE': rmse,\n",
    "                'MSE': mse,\n",
    "                'MAE': mae,\n",
    "                'MAPE':mape,\n",
    "                'Directional Accuracy': direction_accuracy,\n",
    "                'actual_price':actual_price,\n",
    "                'predictions':predictions\n",
    "            }\n",
    "        print(metrics)\n",
    "\n",
    "       \n",
    "        # df_metrics = pd.DataFrame(metrics)\n",
    "        # metrics.to_excel('model_metrics.xlsx', index=metrics)\n",
    "        print(actual_price)\n",
    "        print(predictions)\n",
    "        df_metrics = pd.DataFrame(metrics)\n",
    "        # df_metrics.to_excel('model_metrics_random_forest84.xlsx', index=metrics)\n",
    "        df_metrics.to_excel(writer, sheet_name=f'Run_{r}', index=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VAR model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from statsmodels.tsa.vector_ar.var_model import VAR\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "import numpy as np\n",
    "df=pd.read_csv(\"gold.csv\")\n",
    "window_size=int(df.shape[0]*0.8)-1   #window-size \n",
    "shuju=np.zeros((len(df)-window_size,2)) #\n",
    "for i in range(len(df)-window_size):\n",
    "    dfnew=df.iloc[0:window_size+i,0:3]  #\n",
    "#     dfnew=df.iloc[0:window_size+i,0:3]#\n",
    "    dfnew['Period']=pd.to_datetime(dfnew['Period'])\n",
    "    dfnew.set_index('Period',inplace=True)\n",
    "    data=dfnew\n",
    "  \n",
    "    def check_stationarity(data):\n",
    "        for column in data.columns:\n",
    "            result = adfuller(data[column])\n",
    "            print(f'ADF Statistic for {column}: {result[0]}')\n",
    "            print(f'p-value: {result[1]}')\n",
    "            print('Critical Values:')\n",
    "            for key, value in result[4].items():\n",
    "                print(f'   {key}: {value}')\n",
    "\n",
    "    check_stationarity(data)\n",
    "\n",
    "\n",
    "    if not all(data.apply(lambda x: adfuller(x)[1] < 0.05)):\n",
    "        data = data.diff().dropna()\n",
    "        print(\"one order diff tackle：\")\n",
    "        print(data)\n",
    "    \n",
    "    # VAR model\n",
    "    model = VAR(data)\n",
    "    #\n",
    "    lag_order=3\n",
    "    result = model.fit()\n",
    "    # forecasting\n",
    "    forecast = result.forecast(data.values[-result.k_ar:], steps=1)\n",
    "    # DataFrame\n",
    "    forecast_df = pd.DataFrame(forecast, index=[data.index[-1] + pd.DateOffset(months=3)], columns=data.columns)\n",
    "    print(\"prediction results：\",forecast_df)\n",
    "    shuju[i,0]=forecast_df.iloc[0,0]\n",
    "    shuju[i,1]=forecast_df.iloc[0,1]\n",
    "# shuju=pd.DataFrame(shuju)\n",
    "# shuju.index=df['Period'].iloc[window_size+1:]\n",
    "varpre=shuju+df.iloc[window_size-1:-1,1:3]\n",
    "shuju=pd.DataFrame(shuju)\n",
    "varpre.index=df['Period'].iloc[window_size:]\n",
    "# varpre.to_csv('varpre-1step12test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  \n",
    "import matplotlib.pyplot as plt  \n",
    "from VMD import VMD  \n",
    "#. some sample parameters for VMD  \n",
    "alpha = 2000       # moderate bandwidth constraint  \n",
    "tau = 0.            # noise-tolerance (no strict fidelity enforcement)  \n",
    "K =2             # 3 modes  \n",
    "DC = 0             # no DC part imposed  \n",
    "init = 1           # initialize omegas uniformly  \n",
    "tol = 1e-7  \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.tsa.seasonal import STL\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.linear_model import( LinearRegression, \n",
    "                                 ElasticNet, \n",
    "                                 Ridge, \n",
    "                                 Lasso, \n",
    "                                 HuberRegressor)\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor,AdaBoostRegressor, BaggingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import numpy as np\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from arch import arch_model\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "gold_prices=pd.read_csv(\"gold.csv\")\n",
    "varpre=pd.read_csv('varpre-1step12test.csv')\n",
    "gold_prices=gold_prices.iloc[:,0:]\n",
    "gold_prices['Period']=pd.to_datetime(gold_prices['Period'])\n",
    "gold_prices.set_index('Period',inplace=True)\n",
    "with pd.ExcelWriter('price-proposed-method.xlsx') as writer:\n",
    "    for r in range(10):  #\n",
    "        # \n",
    "        def create_lag_features(data, lag):\n",
    "            lagged_data = pd.DataFrame(index=data.index)\n",
    "            for col in data.columns:\n",
    "                if col != 'Period':\n",
    "                    for k in range(1, lag + 1):\n",
    "                        lagged_data[f'{col}_Lag_{k}'] = data[col].shift(k)\n",
    "        #             lagged_data=lagged_data.drop(f'{col}_Lag_1',axis=1)\n",
    "            return lagged_data\n",
    "\n",
    "\n",
    "        lag = 3 # \n",
    "        # features = create_lag_features(gold_prices, lag)\n",
    "        feature = gold_prices\n",
    "\n",
    "        window_size = 43  # \n",
    "\n",
    "        \n",
    "        modelss = {\n",
    "            'TrendModel': BaggingRegressor(estimator=KNeighborsRegressor(n_neighbors=5), n_estimators=100),\n",
    "            'SeasonalityModel': BaggingRegressor(estimator=KNeighborsRegressor(n_neighbors=5), n_estimators=100),\n",
    "            'ResidualModel': BaggingRegressor(estimator=KNeighborsRegressor(n_neighbors=5), n_estimators=100),\n",
    "                'TrendModel3': RandomForestRegressor(),\n",
    "            'SeasonalityModel3': RandomForestRegressor(),\n",
    "            'ResidualModel3': RandomForestRegressor(),\n",
    "            'TrendModel4': LinearRegression(),\n",
    "            'SeasonalityModel4': LinearRegression(),\n",
    "            'ResidualModel4': LinearRegression()\n",
    "        #         'Lasso': Lasso(0.5, fit_intercept=False)\n",
    "        }\n",
    "\n",
    "      \n",
    "        # predictions = {model_name: [] for model_name in models}\n",
    "        actual_price = []\n",
    "        direction_correct = 0\n",
    "        predictions=[]\n",
    "        predictionsf=[]\n",
    "        # features=features.fillna(method='bfill')\n",
    "        # features=features.dropna()\n",
    "        # print(features)\n",
    "        for i in range(len(gold_prices['GoldPrice']) - window_size):\n",
    "\n",
    "            # STL decomposition\n",
    "            stl = STL(gold_prices['GoldPrice'].iloc[0:i + window_size])\n",
    "            result = stl.fit()\n",
    "            trend = result.trend\n",
    "            seasonalo = result.seasonal\n",
    "            residualo = result.resid\n",
    "            newdata=seasonalo+residualo\n",
    "            u, u_hat, omega = VMD(newdata, alpha, tau, K, DC, init, tol)\n",
    "            u=u.astype(float)\n",
    "            seasonal = pd.DataFrame(u[1],index=gold_prices.index[0:(i + window_size)])\n",
    "            seasonal.columns = ['seasonal']\n",
    "            residual = pd.DataFrame(u[0],index=gold_prices.index[0:(i + window_size)])\n",
    "            residual.columns = ['residual']\n",
    "\n",
    "\n",
    "            feature['Trend'] = trend\n",
    "            feature['Seasonal'] = seasonal\n",
    "            feature['Residual'] = residual\n",
    "            # after creating features\n",
    "        #     features = create_lag_features(feature, lag)\n",
    "        #     names=['GoldPrice','TrendModel','SeasonalityModel','ResidualModel']\n",
    "        #     features[names] = feature[names]\n",
    "        #     print(feature.shape)\n",
    "\n",
    "#            '''below delete external variables'''\n",
    "#             featuretrend=feature.drop(['GoldPrice','GoldPricede','MAgold','diffgold','MAdemand','diffdemand','Seasonal','Residual'], axis=1)\n",
    "#             featureseasonal=feature.drop(['Trend','Residual'], axis=1)\n",
    "#             featureresidual=feature.drop(['GoldPrice','GoldPricede','MAgold','diffgold','MAdemand','diffdemand','Seasonal','Trend'], axis=1)\n",
    "            \n",
    "            featuretrend=feature.drop(['GoldPricede','MAdemand','diffdemand','Seasonal','Residual'], axis=1)\n",
    "            featureseasonal=feature.drop(['Trend','Residual'], axis=1)\n",
    "            featureresidual=feature.drop(['GoldPricede','MAdemand','diffdemand','Seasonal','Trend'], axis=1)\n",
    "\n",
    "            modelt=ARIMA(trend[0:i + window_size], order=(0, 1, 1))\n",
    "            resultt = modelt.fit()\n",
    "            trend_pred1 = resultt.forecast()[0]\n",
    "\n",
    "            featuretrend = create_lag_features(featuretrend, lag)\n",
    "            featuretrend['Trend']=feature['Trend']\n",
    "        #     featuretrend = featuretrend.drop(featuretrend.head(lag).index)\n",
    "            xtraint=featuretrend.iloc[lag:i + window_size,0:-1]\n",
    "            ytraint=featuretrend.iloc[lag:i + window_size,-1]\n",
    "            xtestt=featuretrend.iloc[i+window_size,0:-1]\n",
    "        #     xtestt=np.array(xtestt).reshape(-1,featuretrend.shape[1]-1)\n",
    "\n",
    "            featureseasonal = create_lag_features(featureseasonal, lag)\n",
    "            featureseasonal['Seasonal']=feature['Seasonal']\n",
    "        #     featuretrend = featuretrend.drop(featuretrend.head(lag).index)\n",
    "            xtrains=featureseasonal.iloc[lag:i + window_size,0:-1]\n",
    "            ytrains=featureseasonal.iloc[lag:i + window_size,-1]\n",
    "            xtests=featureseasonal.iloc[i+window_size,0:-1]\n",
    "        #     xtests=np.array(xtests).reshape(-1,featureseasonal.shape[1]-1)\n",
    "\n",
    "            models=ARIMA(residual[0:i + window_size], order=(0, 1, 1))\n",
    "        #     models=SARIMAX(seasonal[0:i + window_size], order=(1, 1, 1), seasonal_order=(1, 1, 1, 4))\n",
    "            results = models.fit()\n",
    "            residual_pred1 = results.forecast()[0]\n",
    "\n",
    "            featureresidual = create_lag_features(featureresidual, lag)\n",
    "            featureresidual['Residual']=feature['Residual']\n",
    "        #     featuretrend = featuretrend.drop(featuretrend.head(lag).index)\n",
    "            xtrainr=featureresidual.iloc[lag:i + window_size,0:-1]\n",
    "            ytrainr=featureresidual.iloc[lag:i + window_size,-1]\n",
    "            xtestr=featureresidual.iloc[i+window_size,0:-1]\n",
    "        #     xtestr=np.array(xtestr).reshape(-1,featureresidual.shape[1]-1)\n",
    "\n",
    "        #     scaler = MinMaxScaler()\n",
    "        # #             scaler = StandardScaler()\n",
    "        #     X_train = scaler.fit_transform(X_train)\n",
    "        #     X_test = scaler.transform(X_test)\n",
    "        #     scaler1 = MinMaxScaler()\n",
    "        #     y_trend = scaler1.fit_transform(y_trend.values.reshape(-1, 1)).flatten()\n",
    "        #     y_seasonal = scaler1.transform(y_seasonal.values.reshape(-1, 1)).flatten()\n",
    "        #     y_residual = scaler1.transform(y_residual.values.reshape(-1, 1)).flatten()\n",
    "            modelr=ARIMA(seasonal[0:i + window_size], order=(0, 1, 1))\n",
    "        #     models=SARIMAX(seasonal[0:i + window_size], order=(1, 1, 1), seasonal_order=(1, 1, 1, 4))\n",
    "            resultr = modelr.fit()\n",
    "            seasonal_pred1 = resultr.forecast()[0]\n",
    "\n",
    "#           \n",
    "            modelfs = RandomForestRegressor(random_state=10)\n",
    "            modelfs.fit(xtraint,ytraint)\n",
    "            importances = modelfs.feature_importances_\n",
    "            threshold = np.median(importances)\n",
    "            selected_features = xtraint.columns[importances > threshold]\n",
    "            xtraint=xtraint[selected_features]\n",
    "            xtestt=xtestt[selected_features]\n",
    "            xtestt=np.array(xtestt).reshape(-1,xtraint.shape[1])\n",
    "\n",
    "\n",
    "            modelfsr = RandomForestRegressor(random_state=10)\n",
    "            modelfsr.fit(xtrainr,ytrainr)\n",
    "            importancesr = modelfsr.feature_importances_\n",
    "            thresholdr = np.median(importancesr)\n",
    "            selected_featuresr = xtrainr.columns[importancesr > thresholdr]\n",
    "            xtrainr=xtrainr[selected_featuresr]\n",
    "            xtestr=xtestr[selected_featuresr]\n",
    "            xtestr=np.array(xtestr).reshape(-1,xtrainr.shape[1])  \n",
    "\n",
    "            modelfss = RandomForestRegressor(random_state=10)\n",
    "            modelfss.fit(xtrains,ytrains)\n",
    "            importancess = modelfss.feature_importances_\n",
    "            thresholds = np.median(importancess)\n",
    "            selected_featuress = xtrains.columns[importancess > thresholds]\n",
    "            xtrains=xtrains[selected_featuress]\n",
    "            xtests=xtests[selected_featuress]\n",
    "            xtests=np.array(xtests).reshape(-1,xtrains.shape[1])       \n",
    "\n",
    "\n",
    "            modelss['SeasonalityModel'].fit(xtrains, ytrains)\n",
    "            modelss['TrendModel'].fit(xtraint, ytraint)\n",
    "            modelss['ResidualModel'].fit(xtrainr, ytrainr)\n",
    "\n",
    "            # forecasting\n",
    "            trend_pred2 = modelss['TrendModel'].predict(xtestt)\n",
    "            seasonal_pred2 = modelss['SeasonalityModel'].predict(xtests)\n",
    "            residual_pred2 = modelss['ResidualModel'].predict(xtestr)\n",
    "\n",
    "            modelss['SeasonalityModel3'].fit(xtrains, ytrains)\n",
    "            modelss['TrendModel3'].fit(xtraint, ytraint)\n",
    "            modelss['ResidualModel3'].fit(xtrainr, ytrainr)\n",
    "\n",
    "#             modelss['SeasonalityModel4'].fit(xtrains, ytrains)\n",
    "#             modelss['TrendModel4'].fit(xtraint, ytraint)\n",
    "#             modelss['ResidualModel4'].fit(xtrainr, ytrainr)\n",
    "            # \n",
    "\n",
    "            trend_pred3 = modelss['TrendModel3'].predict(xtestt)\n",
    "            seasonal_pred3 = modelss['SeasonalityModel3'].predict(xtests)\n",
    "            residual_pred3 = modelss['ResidualModel3'].predict(xtestr)\n",
    "            \n",
    "#             trend_pred4 = modelss['TrendModel4'].predict(xtestt)\n",
    "#             seasonal_pred4 = modelss['SeasonalityModel4'].predict(xtests)\n",
    "#             residual_pred4 = modelss['ResidualModel4'].predict(xtestr)\n",
    "\n",
    "            #\n",
    "        #     trend_pred = scaler1.inverse_transform(np.array(trend_pred).reshape(-1, 1)).flatten()\n",
    "        #     seasonal_pred = scaler1.inverse_transform(np.array(seasonal_pred).reshape(-1, 1)).flatten()\n",
    "        #     residual_pred = scaler1.inverse_transform(np.array(residual_pred).reshape(-1, 1)).flatten()\n",
    "           \n",
    "           \n",
    "            predicted = trend_pred2+ seasonal_pred1 + residual_pred2\n",
    "#             predicted = (trend_pred2+trend_pred3)/2 +seasonal_pred1 + (residual_pred2+residual_pred3)/2\n",
    "#             predicted = (trend_pred2+ seasonal_pred1 + residual_pred2+trend_pred1+ seasonal_pred1 + residual_pred1+trend_pred3+ seasonal_pred1 + residual_pred3)/3\n",
    "            actual_price.append(gold_prices['GoldPrice'].iloc[i + window_size])\n",
    "\n",
    "              \n",
    "\n",
    "            if i < len(feature) - window_size:\n",
    "                    actual_change = gold_prices['GoldPrice'].iloc[i + window_size ] - gold_prices['GoldPrice'].iloc[i + window_size -1]\n",
    "                    predicted_change = predicted - gold_prices['GoldPrice'].iloc[i + window_size -1]\n",
    "                    if np.sign(actual_change) == np.sign(predicted_change):\n",
    "                        direction_correct += 1\n",
    "\n",
    "            predictions.append(predicted)\n",
    "        predictions = np.concatenate([arr for arr in predictions])\n",
    "        actual_price = np.array(actual_price)\n",
    "        '''weighting ensemble'''\n",
    "#         gold_prices=pd.read_csv(\"gold.csv\")\n",
    "        var=pd.read_csv('varpre-1step12test.csv')\n",
    "        gold_prices=gold_prices.iloc[:,0:]\n",
    "#         gold_prices['Period']=pd.to_datetime(gold_prices['Period'])\n",
    "#         gold_prices.set_index('Period',inplace=True)\n",
    "        \n",
    "        predic=np.zeros([len(gold_prices['GoldPrice']) - window_size,1])\n",
    "        # predicted=[]\n",
    "        # print(var.iloc[i,0])\n",
    "        for i in range(len(actual_price)):\n",
    "\n",
    "            if i==0:\n",
    "                predic[0]=(predictions[i]+var.iloc[i,1])/2\n",
    "            elif i>0:\n",
    "                if np.abs((actual_price[i-1]-predictions[i-1]) / actual_price[i-1]) * 100>4 and np.abs((actual_price[i-1]-var.iloc[i-1,1]) /actual_price[i-1])*100>4:\n",
    "            #                     predicted=(predictions[i]+var.iloc[i,1])/2\n",
    "                    if np.abs(actual_price[i-1]-predictions[i-1])>np.abs(actual_price[i-1]-var.iloc[i-1,1]):\n",
    "                        predicted=var.iloc[i,1]\n",
    "                    else:\n",
    "                        predicted=predictions[i]\n",
    "#                 elif np.abs((actual_price[i-1]-predictions[i-1]) / actual_price[i-1]) * 100>4 and np.abs((actual_price[i-1]-var.iloc[i-1,1]) /gold_prices.iloc[window_size+i,0])*100<=4:\n",
    "#                     predicted=(predictions[i]+var.iloc[i,1])/2\n",
    "#                 elif np.abs((actual_price[i-1]-predictions[i-1]) / actual_price[i-1]) * 100<=4 and np.abs((actual_price[i-1]-var.iloc[i-1,1]) /gold_prices.iloc[window_size+i,0])*100>4:\n",
    "#                     predicted=(predictions[i]+var.iloc[i,1])/2\n",
    "                else:\n",
    "                    a=1/np.abs(actual_price[i-1]-predictions[i-1])\n",
    "                    b=1/np.abs(actual_price[i-1]-var.iloc[i-1,1])\n",
    "                    predicted=(a/(a+b))*predictions[i]+(b/(a+b))*var.iloc[i,1]\n",
    "\n",
    "#             predic[i]=predicted\n",
    "\n",
    "\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "            #set parameters\n",
    "#                     pop = 50 #\n",
    "#                     MaxIter = 500 #\n",
    "#                     dim = 2 #\n",
    "#                     lb = 0*np.ones(dim) #\n",
    "#                     ub = 1*np.ones(dim)#\n",
    "#                     # fitness function selection\n",
    "#                     fobj = fun\n",
    "#                     GbestScore,GbestPositon,Curve = TLBO(pop,dim,lb,ub,MaxIter,fobj,predictions[i-1],var.iloc[i-1,1],gold_prices.iloc[window_size+i,0]) \n",
    "#                     GbestPositonnn = [item for sublist in GbestPositon for item in sublist]\n",
    "#                     a=GbestPositonnn[0]\n",
    "#                     b=1-GbestPositonnn[0]\n",
    "#                     print('best fitness', GbestScore)\n",
    "\n",
    "#                     predicted=a*predictions[i]+b*var.iloc[i,1]\n",
    "#                     print(predicted)\n",
    "            predic[i]=predicted\n",
    "\n",
    "        \n",
    "        predictionsf = [item for sublist in predic for item in sublist]\n",
    "        '''end'''\n",
    "       \n",
    "        tmp=0\n",
    "        for loop in range(1,len(actual_price)):\n",
    "            if(( predictionsf[loop] - actual_price[loop-1 ]) * (actual_price[loop] - actual_price[loop-1])>=0):\n",
    "                tmp=tmp+1;        \n",
    "        Dstat = tmp / (len(actual_price)-1)\n",
    "        \n",
    "        '''D'''\n",
    "        # 计\n",
    "        mse = mean_squared_error(actual_price[1:], predictionsf[1:])\n",
    "        rmse = np.sqrt(mse)\n",
    "        mae = mean_absolute_error(actual_price[1:], predictionsf[1:])\n",
    "        mape = np.mean(np.abs((actual_price[1:] - predictionsf[1:]) / actual_price[1:])) * 100\n",
    "#         total_directions = len(feature) - window_size \n",
    "        \n",
    "   \n",
    "#         direction_accuracy = (direction_correct/ total_directions) * 100\n",
    "\n",
    "        metrics = {\n",
    "                'RMSE': rmse,\n",
    "                'MSE': mse,\n",
    "                'MAE': mae,\n",
    "                'MAPE':mape,\n",
    "                'Directional Accuracy': Dstat,\n",
    "                'actual_price':actual_price,\n",
    "                'predictions':predictions,\n",
    "                'var':np.array(var.iloc[0:,1]),\n",
    "                'predictionsf':predictionsf\n",
    "            }\n",
    "        print(metrics)\n",
    "\n",
    "\n",
    "        # df_metrics = pd.DataFrame(metrics)\n",
    "        # metrics.to_excel('model_metrics.xlsx', index=metrics)\n",
    "        print(actual_price)\n",
    "        print(predictionsf)\n",
    "        df_metrics = pd.DataFrame(metrics)\n",
    "        # df_metrics.to_excel('model_metrics_random_forest84.xlsx', index=metrics)\n",
    "        df_metrics.to_excel(writer, sheet_name=f'Run_{r}', index=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  \n",
    "import matplotlib.pyplot as plt  \n",
    "# from vmdpy import VMD  \n",
    "#. some sample parameters for VMD  \n",
    "alpha = 2000       # moderate bandwidth constraint  \n",
    "tau = 0.            # noise-tolerance (no strict fidelity enforcement)  \n",
    "K =3            # 3 modes  \n",
    "DC = 0             # no DC part imposed  \n",
    "init = 1           # initialize omegas uniformly  \n",
    "tol = 1e-7  \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.tsa.seasonal import STL\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.linear_model import( LinearRegression, \n",
    "                                 ElasticNet, \n",
    "                                 Ridge, \n",
    "                                 Lasso, \n",
    "                                 HuberRegressor)\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor,AdaBoostRegressor, BaggingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import numpy as np\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from arch import arch_model\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "gold_prices=pd.read_csv(\"gold.csv\")\n",
    "# varpre=pd.read_csv('varpre-1step12test.csv')\n",
    "gold_prices=gold_prices.iloc[:,0:]\n",
    "gold_prices['Period']=pd.to_datetime(gold_prices['Period'])\n",
    "gold_prices.set_index('Period',inplace=True)\n",
    "with pd.ExcelWriter('demand-proposed.xlsx') as writer:\n",
    "    for r in range(10):  \n",
    "        def create_lag_features(data, lag):\n",
    "            lagged_data = pd.DataFrame(index=data.index)\n",
    "            for col in data.columns:\n",
    "                if col != 'Period':\n",
    "                    for k in range(1, lag + 1):\n",
    "                        lagged_data[f'{col}_Lag_{k}'] = data[col].shift(k)\n",
    "        #             lagged_data=lagged_data.drop(f'{col}_Lag_1',axis=1)\n",
    "            return lagged_data\n",
    "\n",
    "\n",
    "        lag = 3 # \n",
    "        # features = create_lag_features(gold_prices, lag)\n",
    "        feature = gold_prices\n",
    "\n",
    "        window_size = 43 # \n",
    "\n",
    "   \n",
    "        modelss = {\n",
    "            'TrendModel': BaggingRegressor(estimator=KNeighborsRegressor(n_neighbors=5), n_estimators=100),\n",
    "            'SeasonalityModel': BaggingRegressor(estimator=KNeighborsRegressor(n_neighbors=5), n_estimators=100),\n",
    "            'ResidualModel': BaggingRegressor(estimator=KNeighborsRegressor(n_neighbors=5), n_estimators=100),\n",
    "            'lasso1':RandomForestRegressor(),\n",
    "            'lasso2': RandomForestRegressor(),\n",
    "            'lasso3':RandomForestRegressor()\n",
    "        }\n",
    "\n",
    "   \n",
    "        # predictions = {model_name: [] for model_name in models}\n",
    "        actual_price = []\n",
    "        direction_correct = 0\n",
    "        predictions=[]\n",
    "        predictionsf=[]\n",
    "        # features=features.fillna(method='bfill')\n",
    "        # features=features.dropna()\n",
    "        # print(features)\n",
    "        for i in range(len(gold_prices['GoldPricede']) - window_size):\n",
    "\n",
    "\n",
    "            u, u_hat, omega = VMD(gold_prices['GoldPricede'].iloc[0:i + window_size], alpha, tau, K, DC, init, tol)\n",
    "            u=u.astype(float)\n",
    "            trend = pd.DataFrame(u[0],index=gold_prices.index[0:(i + window_size)])\n",
    "            trend.columns = ['trend']\n",
    "\n",
    "            newdata = pd.DataFrame(gold_prices['GoldPricede'].iloc[0:i + window_size]-u[0],index=gold_prices.index[0:(i + window_size)])\n",
    "            stl = STL(newdata)\n",
    "            result = stl.fit()\n",
    "            seasonal = result.seasonal+result.resid\n",
    "            residual = result.trend\n",
    "\n",
    "\n",
    "            feature['Trend'] = trend\n",
    "            feature['Seasonal'] = seasonal\n",
    "            feature['Residual'] = residual\n",
    "          \n",
    "        #     features = create_lag_features(feature, lag)\n",
    "        #     names=['GoldPrice','TrendModel','SeasonalityModel','ResidualModel']\n",
    "        #     features[names] = feature[names]\n",
    "        #     print(feature.shape)\n",
    "\n",
    "        #     print(features.shape)\n",
    "            featuretrend=feature.drop(['GoldPrice','GoldPricede','MAgold','diffgold','MAdemand','diffdemand','Seasonal','Residual'], axis=1)\n",
    "            featureseasonal=feature.drop(['GoldPricede','MAdemand','diffdemand','Trend','Residual'], axis=1)\n",
    "            featureresidual=feature.drop(['GoldPricede','MAdemand','diffdemand','Seasonal','Trend'], axis=1)\n",
    "            #'''no external variables'''\n",
    "#             featuretrend=feature.drop(['GoldPrice','GoldPricede','MAgold','diffgold','MAdemand','diffdemand','Seasonal','Residual'], axis=1)\n",
    "#             featureseasonal=feature.drop(['GoldPrice','GoldPricede','MAgold','diffgold','MAdemand','diffdemand','Trend','Residual'], axis=1)\n",
    "#             featureresidual=feature.drop(['GoldPrice','GoldPricede','MAgold','diffgold','MAdemand','diffdemand','Seasonal','Trend'], axis=1)\n",
    "\n",
    "            modelt=ARIMA(trend[0:i + window_size], order=(0, 1, 1))\n",
    "            resultt = modelt.fit()\n",
    "            trend_pred1 = resultt.forecast()[0]\n",
    "\n",
    "            featuretrend = create_lag_features(featuretrend, lag)\n",
    "            featuretrend['Trend']=feature['Trend']\n",
    "        #     featuretrend = featuretrend.drop(featuretrend.head(lag).index)\n",
    "            xtraint=featuretrend.iloc[lag:i + window_size,0:-1]\n",
    "            ytraint=featuretrend.iloc[lag:i + window_size,-1]\n",
    "            xtestt=featuretrend.iloc[i+window_size,0:-1]\n",
    "            xtestt=np.array(xtestt).reshape(-1,featuretrend.shape[1]-1)\n",
    "\n",
    "            featureseasonal = create_lag_features(featureseasonal, lag)\n",
    "            featureseasonal['Seasonal']=feature['Seasonal']\n",
    "        #     featuretrend = featuretrend.drop(featuretrend.head(lag).index)\n",
    "            xtrains=featureseasonal.iloc[lag:i + window_size,0:-1]\n",
    "            ytrains=featureseasonal.iloc[lag:i + window_size,-1]\n",
    "            xtests=featureseasonal.iloc[i+window_size,0:-1]\n",
    "        #     xtests=np.array(xtests).reshape(-1,featureseasonal.shape[1]-1)\n",
    "\n",
    "            models=ARIMA(seasonal[0:i + window_size], order=(0, 1, 1))\n",
    "        #     models=SARIMAX(seasonal[0:i + window_size], order=(1, 1, 1), seasonal_order=(1, 1, 1, 4))\n",
    "            results = models.fit()\n",
    "            seasonal_pred1 = results.forecast()[0]\n",
    "\n",
    "            featureresidual = create_lag_features(featureresidual, lag)\n",
    "            featureresidual['Residual']=feature['Residual']\n",
    "        #     featuretrend = featuretrend.drop(featuretrend.head(lag).index)\n",
    "            xtrainr=featureresidual.iloc[lag:i + window_size,0:-1]\n",
    "            ytrainr=featureresidual.iloc[lag:i + window_size,-1]\n",
    "            xtestr=featureresidual.iloc[i+window_size,0:-1]\n",
    "\n",
    "\n",
    "            models=ARIMA(residual[0:i + window_size], order=(0, 1, 1))\n",
    "        #     models=SARIMAX(seasonal[0:i + window_size], order=(1, 1, 1), seasonal_order=(1, 1, 1, 4))\n",
    "            results = models.fit()\n",
    "            residual_pred1 = results.forecast()[0]\n",
    "\n",
    "        #     xtestr=np.array(xtestr).reshape(-1,featureresidual.shape[1]-1)\n",
    "\n",
    "        #     scaler = MinMaxScaler()\n",
    "        # #             scaler = StandardScaler()\n",
    "        #     X_train = scaler.fit_transform(X_train)\n",
    "        #     X_test = scaler.transform(X_test)\n",
    "        #     scaler1 = MinMaxScaler()\n",
    "        #     y_trend = scaler1.fit_transform(y_trend.values.reshape(-1, 1)).flatten()\n",
    "        #     y_seasonal = scaler1.transform(y_seasonal.values.reshape(-1, 1)).flatten()\n",
    "        #     y_residual = scaler1.transform(y_residual.values.reshape(-1, 1)).flatten()\n",
    "\n",
    "       \n",
    "            modelfs = RandomForestRegressor(random_state=0)\n",
    "            modelfs.fit(xtrains,ytrains)\n",
    "            importances = modelfs.feature_importances_\n",
    "            threshold = np.median(importances)\n",
    "            selected_features = xtrains.columns[importances > threshold]\n",
    "            xtrains=xtrains[selected_features]\n",
    "            xtests=xtests[selected_features]\n",
    "            xtests=np.array(xtests).reshape(-1,xtrains.shape[1])\n",
    "\n",
    "        \n",
    "            modelfsr = RandomForestRegressor(random_state=0)\n",
    "            modelfsr.fit(xtrainr,ytrainr)\n",
    "            importancesr = modelfsr.feature_importances_\n",
    "            thresholdr = np.median(importances)\n",
    "            selected_featuresr = xtrainr.columns[importancesr > thresholdr]\n",
    "            xtrainr=xtrainr[selected_featuresr]\n",
    "            xtestr=xtestr[selected_featuresr]\n",
    "            xtestr=np.array(xtestr).reshape(-1,xtrainr.shape[1])  \n",
    "\n",
    "            modelss['SeasonalityModel'].fit(xtrains, ytrains)\n",
    "            modelss['TrendModel'].fit(xtraint, ytraint)\n",
    "            modelss['ResidualModel'].fit(xtrainr, ytrainr)\n",
    "\n",
    "            modelss['lasso2'].fit(xtrains, ytrains)\n",
    "            modelss['lasso1'].fit(xtraint, ytraint)\n",
    "            modelss['lasso3'].fit(xtrainr, ytrainr)\n",
    "            # \n",
    "\n",
    "            trend_pred2 = modelss['TrendModel'].predict(xtestt)\n",
    "            seasonal_pred2 = modelss['SeasonalityModel'].predict(xtests)\n",
    "            residual_pred2 = modelss['ResidualModel'].predict(xtestr)\n",
    "            \n",
    "            trend_pred3 = modelss['lasso1'].predict(xtestt)\n",
    "            seasonal_pred3 = modelss['lasso2'].predict(xtests)\n",
    "            residual_pred3 = modelss['lasso3'].predict(xtestr)\n",
    "\n",
    "            #\n",
    "        #     trend_pred = scaler1.inverse_transform(np.array(trend_pred).reshape(-1, 1)).flatten()\n",
    "        #     seasonal_pred = scaler1.inverse_transform(np.array(seasonal_pred).reshape(-1, 1)).flatten()\n",
    "        #     residual_pred = scaler1.inverse_transform(np.array(residual_pred).reshape(-1, 1)).flatten()\n",
    "            \n",
    "# \n",
    "            predicted = trend_pred1+ seasonal_pred2 + residual_pred2\n",
    "#             predicted = (trend_pred1+ seasonal_pred2 + residual_pred2+trend_pred1+ seasonal_pred3 + residual_pred3)/2\n",
    "\n",
    "        \n",
    "            actual_price.append(gold_prices['GoldPricede'].iloc[i + window_size])\n",
    "#             predicted=0.9*predicted+0.1*varpre.iloc[i,2]    \n",
    "            #     predicted=0.98*predicted+0.02*varpre.iloc[i+1,2] \n",
    "#             if i < len(feature) - window_size:\n",
    "#                     actual_change = gold_prices['GoldPricede'].iloc[i + window_size ] - gold_prices['GoldPricede'].iloc[i + window_size -1]\n",
    "#                     predicted_change = predicted - gold_prices['GoldPricede'].iloc[i + window_size -1]\n",
    "#                     if np.sign(actual_change) == np.sign(predicted_change):\n",
    "#                         direction_correct += 1\n",
    "\n",
    "            predictions.append(predicted)\n",
    "        predictions = np.concatenate([arr for arr in predictions])\n",
    "        actual_price = np.array(actual_price)\n",
    "        '''weight ensemble'''\n",
    "#         gold_prices=pd.read_csv(\"gold.csv\")\n",
    "        var=pd.read_csv('varpre-1step12test.csv')\n",
    "        gold_prices=gold_prices.iloc[:,0:]\n",
    "#         gold_prices['Period']=pd.to_datetime(gold_prices['Period'])\n",
    "#         gold_prices.set_index('Period',inplace=True)\n",
    "        \n",
    "        predic=np.zeros([len(gold_prices['GoldPrice']) - window_size,1])\n",
    "        # predicted=[]\n",
    "        # print(var.iloc[i,0])\n",
    "        for i in range(len(actual_price)):\n",
    "\n",
    "            if i==0:\n",
    "                predic[0]=(predictions[i]+var.iloc[i,2])/2\n",
    "            elif i>0:\n",
    "                if np.abs((actual_price[i-1]-predictions[i-1]) / actual_price[i-1]) * 100>10 and np.abs((actual_price[i-1]-var.iloc[i-1,2]) /actual_price[i-1])*100>10:\n",
    "            #                     predicted=(predictions[i]+var.iloc[i,1])/2\n",
    "                    if np.abs(actual_price[i-1]-predictions[i-1])>np.abs(actual_price[i-1]-var.iloc[i-1,2]):\n",
    "                        predicted=var.iloc[i,2]\n",
    "                    else:\n",
    "                        predicted=predictions[i]\n",
    "#                 elif np.abs((actual_price[i-1]-predictions[i-1]) / actual_price[i-1]) * 100>10 and np.abs((actual_price[i-1]-var.iloc[i-1,2]) /gold_prices.iloc[window_size+i,0])*100<=10:\n",
    "#                     predicted=(predictions[i]+var.iloc[i,2])/2\n",
    "#                 elif np.abs((actual_price[i-1]-predictions[i-1]) / actual_price[i-1]) * 100<=10 and np.abs((actual_price[i-1]-var.iloc[i-1,2]) /gold_prices.iloc[window_size+i,0])*100>10:\n",
    "#                     predicted=(predictions[i]+var.iloc[i,2])/2\n",
    "                else:\n",
    "                    a=1/np.abs(actual_price[i-1]-predictions[i-1])\n",
    "                    b=1/np.abs(actual_price[i-1]-var.iloc[i-1,2])\n",
    "                    predicted=(a/(a+b))*predictions[i]+(b/(a+b))*var.iloc[i,2]\n",
    "\n",
    "#             predic[i]=predicted\n",
    "\n",
    "\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "            #set parameters\n",
    "#                     pop = 50 #\n",
    "#                     MaxIter = 500 \n",
    "#                     dim = 2 #\n",
    "#                     lb = 0*np.ones(dim) \n",
    "#                     ub = 1*np.ones(dim)\n",
    "#                \n",
    "#                     fobj = fun\n",
    "#                     GbestScore,GbestPositon,Curve = TLBO(pop,dim,lb,ub,MaxIter,fobj,predictions[i-1],var.iloc[i-1,1],gold_prices.iloc[window_size+i,0]) \n",
    "#                     GbestPositonnn = [item for sublist in GbestPositon for item in sublist]\n",
    "#                     a=GbestPositonnn[0]\n",
    "#                     b=1-GbestPositonnn[0]\n",
    "#                     print('bet fitness', GbestScore)\n",
    "\n",
    "#                     predicted=a*predictions[i]+b*var.iloc[i,1]\n",
    "#                     print(predicted)\n",
    "            predic[i]=predicted\n",
    "\n",
    "        \n",
    "        predictionsf = [item for sublist in predic for item in sublist]\n",
    "        '''end'''\n",
    "      \n",
    "        tmp=0\n",
    "        for loop in range(1,len(actual_price)):\n",
    "            if(( predictionsf[loop] - actual_price[loop-1 ]) * (actual_price[loop] - actual_price[loop-1])>=0):\n",
    "                tmp=tmp+1;        \n",
    "        Dstat = tmp / (len(actual_price)-1)\n",
    "        \n",
    "\n",
    "        mse = mean_squared_error(actual_price[1:], predictionsf[1:])\n",
    "        rmse = np.sqrt(mse)\n",
    "        mae = mean_absolute_error(actual_price[1:], predictionsf[1:])\n",
    "        mape = np.mean(np.abs((actual_price[1:] - predictionsf[1:]) / actual_price[1:])) * 100\n",
    "#         total_directions = len(feature) - window_size \n",
    "        \n",
    "\n",
    "#         direction_accuracy = (direction_correct/ total_directions) * 100\n",
    "\n",
    "        metrics = {\n",
    "                'RMSE': rmse,\n",
    "                'MSE': mse,\n",
    "                'MAE': mae,\n",
    "                'MAPE':mape,\n",
    "                'Directional Accuracy': Dstat,\n",
    "                'actual_price':actual_price,\n",
    "                'predictions':predictions,\n",
    "                'var':np.array(var.iloc[0:,2]),\n",
    "                'predictionsf':predictionsf\n",
    "            }\n",
    "        print(metrics)\n",
    "\n",
    "\n",
    "        # df_metrics = pd.DataFrame(metrics)\n",
    "        # metrics.to_excel('model_metrics.xlsx', index=metrics)\n",
    "        print(actual_price)\n",
    "        print(predictionsf)\n",
    "        df_metrics = pd.DataFrame(metrics)\n",
    "        # df_metrics.to_excel('model_metrics_random_forest84.xlsx', index=metrics)\n",
    "        df_metrics.to_excel(writer, sheet_name=f'Run_{r}', index=metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lowtsdf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
