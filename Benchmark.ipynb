{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "from tensorflow.keras.layers import Input,Add,Flatten, Dense, LayerNormalization, MultiHeadAttention, Dropout, GlobalAveragePooling1D,LSTM\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import Sequential\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'golddemandQ.csv'  # Update this to the correct path where IBM.csv is located\n",
    "data = pd.read_csv(data_path)\n",
    "data=data.iloc[:,0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.simplefilter(\"ignore\", category=DeprecationWarning)\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow import keras\n",
    "from keras.activations import get\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "import matplotlib.pyplot as plt\n",
    "# Preprocess data\n",
    "# Set random seed for reproducibility\n",
    "with pd.ExcelWriter('transformer-demand-mult_sheets.xlsx') as writer:\n",
    "    for r in range(10):  # 循环运行10次\n",
    "\n",
    "\n",
    "        data_path = 'golddemandQ.csv'  # Update this to the correct path where IBM.csv is located\n",
    "        data = pd.read_csv(data_path)\n",
    "        data=data.iloc[:,[0,2]]\n",
    "\n",
    "        np.random.seed(42)\n",
    "        torch.manual_seed(42)\n",
    "        tf.random.set_seed(42)\n",
    "\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        from sklearn.preprocessing import MinMaxScaler\n",
    "        from tensorflow.keras.layers import Input, Dense, MultiHeadAttention, Dropout, LayerNormalization, Add, Flatten\n",
    "        from tensorflow.keras.models import Model\n",
    "        import matplotlib.pyplot as plt\n",
    "        from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "\n",
    "\n",
    "\n",
    "        # Display first few rows of the dataset\n",
    "        print(data.head())\n",
    "\n",
    "        # Ensure the 'Date' column is datetime type and set it as index if necessary\n",
    "        # data['time'] = pd.to_datetime(data['time'])\n",
    "        data.set_index('time', inplace=True)\n",
    "\n",
    "        # Use 'Close' price for prediction\n",
    "        data = data[['closed']]\n",
    "\n",
    "\n",
    "\n",
    "        def create_dataset(data, time_step=2):\n",
    "            X, y = [], []\n",
    "            for i in range(len(data) - time_step):\n",
    "                X.append(data[i:i + time_step])\n",
    "                y.append(data[i + time_step])\n",
    "            return np.array(X), np.array(y)\n",
    "\n",
    "        # Create dataset\n",
    "        X, y = create_dataset(data[['closed']].values, time_step)\n",
    "\n",
    "        # Split data into training and testing sets (80% for training, 20% for testing)\n",
    "        train_size = int(len(X) * 0.8)\n",
    "        X_train0, y_train0 = X[:train_size], y[:train_size]\n",
    "        X_test0, y_test0 = X[train_size:], y[train_size:]\n",
    "\n",
    "        # Standardize the training data\n",
    "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        X_train= scaler.fit_transform(X_train0.reshape(-1, 1)).reshape(X_train0.shape)\n",
    "        y_train = scaler.fit_transform(y_train0.reshape(-1, 1)).reshape(-1)\n",
    "\n",
    "        # Apply the same scaling to the test data\n",
    "        X_test = scaler.transform(X_test0.reshape(-1, 1)).reshape(X_test0.shape)\n",
    "        y_test = scaler.transform(y_test0.reshape(-1, 1)).reshape(-1)\n",
    "        # Build the Transformer model\n",
    "        def build_transformer_model(input_shape):\n",
    "            inputs = Input(shape=input_shape)\n",
    "\n",
    "            # Transformer encoder layer\n",
    "            attention = MultiHeadAttention(num_heads=8, key_dim=32)(inputs, inputs)\n",
    "            attention = Dropout(0.2)(attention)\n",
    "            attention = LayerNormalization(epsilon=1e-6)(attention)\n",
    "\n",
    "            # Add residual connection\n",
    "            x = Add()([inputs, attention])\n",
    "\n",
    "            # Dense layers\n",
    "            x = Flatten()(x)\n",
    "            x = Dense(128, activation='relu')(x)\n",
    "            x = Dropout(0.3)(x)\n",
    "            x = Dense(64, activation='relu')(x)\n",
    "            outputs = Dense(1)(x)\n",
    "\n",
    "            model = Model(inputs=inputs, outputs=outputs)\n",
    "            return model\n",
    "\n",
    "        # Initialize and compile the model\n",
    "        input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "        transformer_model = build_transformer_model(input_shape)\n",
    "        transformer_model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "        # Train the model\n",
    "        history = transformer_model.fit(\n",
    "            X_train, y_train,\n",
    "            validation_data=(X_test, y_test),\n",
    "            epochs=50,\n",
    "            batch_size=1,\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        # Predict on the test set\n",
    "        transformer_predictions = transformer_model.predict(X_test)\n",
    "        transformer_predictions = scaler.inverse_transform(transformer_predictions)\n",
    "        y_test_actual = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
    "\n",
    "        # Evaluate the model\n",
    "        transformer_r2 = r2_score(y_test_actual, transformer_predictions)\n",
    "        transformer_rmse = np.sqrt(mean_squared_error(y_test_actual, transformer_predictions))\n",
    "        transformer_mae = mean_absolute_error(y_test_actual, transformer_predictions)\n",
    "\n",
    "        # Evaluate the model using MAE, RMSE, MAPE, and Directional Accuracy\n",
    "        def calculate_mape(y_true, y_pred):\n",
    "            return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "        def calculate_directional_accuracy(y_true, y_pred):\n",
    "            correct = np.sum(np.sign(y_true) == np.sign(y_pred))\n",
    "            return correct / len(y_true)\n",
    "\n",
    "        transformer_mape = calculate_mape(y_test_actual, transformer_predictions)\n",
    "        direction_test = calculate_directional_accuracy(y_test_actual, transformer_predictions)\n",
    "\n",
    "        # Print evaluation metrics\n",
    "        print(f\"Transformer R² Value: {transformer_r2:.4f}\")\n",
    "        print(f\"Transformer RMSE: {transformer_rmse:.4f}\")\n",
    "        print(f\"Transformer MAE: {transformer_mae:.4f}\")\n",
    "        print(f\"Transformer mape: {transformer_mape:.4f}\")\n",
    "\n",
    "\n",
    "        metrics1 = {\n",
    "                'RMSE': transformer_rmse,\n",
    "                # 'MSE': mse,\n",
    "                'MAE': transformer_mae,\n",
    "                'MAPE':transformer_mape,\n",
    "                # 'Directional Accuracy': direction_accuracy,\n",
    "                # 'actual_price':y_test_actual,\n",
    "                # 'predictions': transformer_predictions\n",
    "            }\n",
    "        # print(metrics1)\n",
    "        metrics2={'actual_price':np.squeeze(y_test_actual),\n",
    "                'predictions': np.squeeze(transformer_predictions)}\n",
    "     \n",
    "        # df_metrics = pd.DataFrame([metrics],index=[0])\n",
    "        # metrics.to_excel('model_metrics.xlsx', index=metrics)\n",
    "        df_metrics1 = pd.DataFrame(metrics1,index=[0])\n",
    "        df_metrics2 = pd.DataFrame(metrics2)\n",
    "        df_metrics=pd.concat([df_metrics1,df_metrics2])\n",
    "        # df_metrics.to_excel('model_metrics_random_forest84.xlsx', index=metrics)\n",
    "        df_metrics.to_excel(writer, sheet_name=f'Run_{r}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MIMO\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor,AdaBoostRegressor, BaggingRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "# Create a random dataset\n",
    "rng = np.random.RandomState(1)\n",
    "data_path = 'golddemandQ.csv'  # Update this to the correct path where IBM.csv is located\n",
    "data = pd.read_csv(data_path)\n",
    "data=data.iloc[:,:]\n",
    "\n",
    "data.set_index('time', inplace=True)\n",
    "\n",
    "data_close = data[['closep','closed']].values\n",
    "split = int(len(data_close) * 0.8)\n",
    "data_close_train, data_close_test = data_close[:split], data_close[split:]\n",
    "\n",
    "def create_sequences(data, sequence_length, steps):\n",
    "    xs, ys = [], []\n",
    "    for i in range(len(data) - sequence_length - steps + 1):\n",
    "        x = data[i:(i + sequence_length)]\n",
    "        y = data[i + sequence_length:i + sequence_length + steps]\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "    return np.array(xs), np.array(ys)\n",
    "\n",
    "def create_sequencestest(data, sequence_length, steps,price_data_test,split):\n",
    "    xs, ys = [], []\n",
    "    for i in range(len(price_data_test)):\n",
    "        x = data[(split-sequence_length)+i:split+i]\n",
    "        y = data[split+i:split+i+ steps]\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "    return np.array(xs), np.array(ys)\n",
    "\n",
    "# Create sequences with modified function\n",
    "sequence_length = 2  # Updated sequence length for your use case\n",
    "steps = 1  # Number of future steps to predict\n",
    "\n",
    "\n",
    "\n",
    "X_train, y_train = create_sequences(data_close_train, sequence_length, steps)\n",
    "X_test, y_test = create_sequencestest(data_close, sequence_length, steps,price_data_test,split)\n",
    "\n",
    "X_train=X_train.reshape(-1,2*sequence_length)\n",
    "y_train = y_train.reshape(-1,1*sequence_length)\n",
    "X_test=X_test.reshape(-1,2*sequence_length)\n",
    "y_test = y_test.reshape(-1,1*sequence_length)\n",
    "\n",
    "max_depth = 50\n",
    "# regr_multirf = MultiOutputRegressor(BaggingRegressor(estimator=KNeighborsRegressor(n_neighbors=5), n_estimators=100))\n",
    "regr_multirf = MultiOutputRegressor(RandomForestRegressor(n_estimators=10,\n",
    "                                                          max_depth=max_depth,\n",
    "                                                          random_state=0))\n",
    "regr_multirf.fit(X_train, y_train)\n",
    "\n",
    "regr_rf = RandomForestRegressor(n_estimators=10, max_depth=max_depth,\n",
    "                                random_state=2)\n",
    "regr_rf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on new data\n",
    "y_multirf = regr_multirf.predict(X_test)\n",
    "y_rf = regr_rf.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test[:,0], y_multirf[:,0])\n",
    "rmse = np.sqrt(mean_squared_error(y_test[:,0], y_multirf[:,0]))\n",
    "\n",
    "def calculate_mape(y_true, y_pred):\n",
    "    return np.mean(np.abs(((y_true - y_pred) / y_true))) * 100\n",
    "\n",
    "mape = calculate_mape(y_test[:,0], y_multirf[:,0])\n",
    "\n",
    "\n",
    "print('''first-variable-MIMOresults''')\n",
    "print(f'MAE: {np.mean(mae)}')\n",
    "print(f'RMSE: {np.mean(rmse)}')\n",
    "print(f'MAPE: {mape}%')\n",
    "mae2 = mean_absolute_error(y_test[:,1], y_multirf[:,1])\n",
    "rmse2 = np.sqrt(mean_squared_error(y_test[:,1], y_multirf[:,1]))\n",
    "def calculate_mape(y_true, y_pred):\n",
    "    return np.mean(np.abs(((y_true - y_pred) / y_true))) * 100\n",
    "\n",
    "mape2 = calculate_mape(y_test[:,1], y_multirf[:,1])\n",
    "\n",
    "\n",
    "print('''second-variable-MIMOresults''')\n",
    "print(f'MAE: {np.mean(mae2)}')\n",
    "print(f'RMSE: {np.mean(rmse2)}')\n",
    "print(f'MAPE: {mape2}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LSTM\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "with pd.ExcelWriter('LSTM-demand-mult_sheets.xlsx') as writer:\n",
    "    for r in range(10):  \n",
    "        # Set random seed for reproducibility\n",
    "        np.random.seed(42)\n",
    "\n",
    "        # Load the data\n",
    "        data_path = 'golddemandQ.csv'  # Update this to the correct path where IBM.csv is located\n",
    "        data = pd.read_csv(data_path)\n",
    "        data=data.iloc[:,[0,2]]\n",
    "\n",
    "        def create_dataset(data, time_step=2):\n",
    "            X, y = [], []\n",
    "            for i in range(len(data) - time_step):\n",
    "                X.append(data[i:i + time_step])\n",
    "                y.append(data[i + time_step])\n",
    "            return np.array(X), np.array(y)\n",
    "\n",
    "        # Create dataset\n",
    "        X, y = create_dataset(data[['closed']].values, time_step)\n",
    "\n",
    "        # Split data into training and testing sets (80% for training, 20% for testing)\n",
    "        train_size = int(len(X) * 0.8)\n",
    "        X_train0, y_train0 = X[:train_size], y[:train_size]\n",
    "        X_test0, y_test0 = X[train_size:], y[train_size:]\n",
    "\n",
    "        # Standardize the training data\n",
    "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        X_train= scaler.fit_transform(X_train0.reshape(-1, 1)).reshape(X_train0.shape)\n",
    "        y_train = scaler.fit_transform(y_train0.reshape(-1, 1)).reshape(-1)\n",
    "\n",
    "        # Apply the same scaling to the test data\n",
    "        X_test = scaler.transform(X_test0.reshape(-1, 1)).reshape(X_test0.shape)\n",
    "        y_test = scaler.transform(y_test0.reshape(-1, 1)).reshape(-1)\n",
    "\n",
    "        # Build the LSTM model\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(128, input_shape=(sequence_length, 1)))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(steps))  # The output layer should have 'steps' units if predicting multiple steps\n",
    "\n",
    "        # Compile the model\n",
    "        model.compile(optimizer=Adam(0.001), loss='mean_squared_error')\n",
    "\n",
    "        # Train the model\n",
    "        model.fit(X_train, y_train, epochs=50, batch_size=1, verbose=1, validation_data=(X_test, y_test))\n",
    "\n",
    "        # Predicting\n",
    "        train_predict = model.predict(X_train)\n",
    "        test_predict = model.predict(X_test)\n",
    "\n",
    "        # Inverse transform the predictions back to original scale\n",
    "        train_predict = scaler.inverse_transform(train_predict)\n",
    "        test_predict = scaler.inverse_transform(test_predict)\n",
    "        y_train_inv = scaler.inverse_transform(y_train.reshape(-1, 1))\n",
    "        y_test_inv = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
    "\n",
    "        # Evaluate the model using MAE, RMSE, MAPE, and Directional Accuracy\n",
    "        def calculate_mape(y_true, y_pred):\n",
    "            return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "        def calculate_directional_accuracy(y_true, y_pred):\n",
    "            correct = np.sum(np.sign(y_true) == np.sign(y_pred))\n",
    "            return correct / len(y_true)\n",
    "\n",
    "        mae_train = mean_absolute_error(y_train_inv, train_predict)\n",
    "        rmse_train = np.sqrt(mean_squared_error(y_train_inv, train_predict))\n",
    "        mape_train = calculate_mape(y_train_inv, train_predict)\n",
    "        direction_train = calculate_directional_accuracy(y_train_inv, train_predict)\n",
    "\n",
    "        mae_test = mean_absolute_error(y_test_inv, test_predict)\n",
    "        rmse_test = np.sqrt(mean_squared_error(y_test_inv, test_predict))\n",
    "        mape_test = calculate_mape(y_test_inv, test_predict)\n",
    "        direction_test = calculate_directional_accuracy(y_test_inv, test_predict)\n",
    "\n",
    "        print(f\"Training Data - MAE: {mae_train}, RMSE: {rmse_train}, MAPE: {mape_train}%, Directional Accuracy: {direction_train}\")\n",
    "        print(f\"Testing Data - MAE: {mae_test}, RMSE: {rmse_test}, MAPE: {mape_test}%, Directional Accuracy: {direction_test}\")\n",
    "\n",
    "        metrics1 = {\n",
    "                'RMSE': rmse_test,\n",
    "                # 'MSE': mse,\n",
    "                'MAE': mae_test,\n",
    "                'MAPE':mape_test,\n",
    "                # 'Directional Accuracy': direction_accuracy,\n",
    "                # 'actual_price':y_test_actual,\n",
    "                # 'predictions': transformer_predictions\n",
    "            }\n",
    "        # print(metrics1)\n",
    "        metrics2={'actual_price':np.squeeze(y_test_inv),\n",
    "                'predictions': np.squeeze(test_predict)}\n",
    "   \n",
    "        # df_metrics = pd.DataFrame([metrics],index=[0])\n",
    "        # metrics.to_excel('model_metrics.xlsx', index=metrics)\n",
    "        df_metrics1 = pd.DataFrame(metrics1,index=[0])\n",
    "        df_metrics2 = pd.DataFrame(metrics2)\n",
    "        df_metrics=pd.concat([df_metrics1,df_metrics2])\n",
    "        # df_metrics.to_excel('model_metrics_random_forest84.xlsx', index=metrics)\n",
    "        df_metrics.to_excel(writer, sheet_name=f'Run_{r}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN-LSTM预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CNN-LSTM\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Conv1D, MaxPooling1D, Flatten, TimeDistributed\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "with pd.ExcelWriter('CNNLSTM-demand-mult_sheets.xlsx') as writer:\n",
    "    for r in range(1):  # \n",
    "        # Set random seed for reproduciility\n",
    "        np.random.seed(42)\n",
    "\n",
    "        \n",
    "        # Set random seed for reproducibility\n",
    "        np.random.seed(42)\n",
    "\n",
    "        # Load the data from a CSV file\n",
    "        data_path = 'golddemandQ.csv'  # Update this to the correct path where IBM.csv is located\n",
    "        data = pd.read_csv(data_path)\n",
    "        data=data.iloc[:,0:2]\n",
    "        data.set_index('time', inplace=True)\n",
    "\n",
    "        # Use 'Close' price for prediction\n",
    "        data = data[['closep']]\n",
    "\n",
    "        def create_dataset(data, time_step=4):\n",
    "            X, y = [], []\n",
    "            for i in range(len(data) - time_step):\n",
    "                X.append(data[i:i + time_step])\n",
    "                y.append(data[i + time_step])\n",
    "            return np.array(X), np.array(y)\n",
    "\n",
    "        # Create dataset\n",
    "        X, y = create_dataset(data[['closep']].values, time_step=4)\n",
    "\n",
    "        # Split data into training and testing sets (80% for training, 20% for testing)\n",
    "        train_size = int(len(X) * 0.8)\n",
    "        X_train0, y_train0 = X[:train_size], y[:train_size]\n",
    "        X_test0, y_test0 = X[train_size:], y[train_size:]\n",
    "\n",
    "        # Standardize the training data\n",
    "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        X_train= scaler.fit_transform(X_train0.reshape(-1, 1)).reshape(X_train0.shape)\n",
    "        y_train = scaler.fit_transform(y_train0.reshape(-1, 1)).reshape(-1)\n",
    "\n",
    "        # # Apply the same scaling to the test data\n",
    "        X_test = scaler.transform(X_test0.reshape(-1, 1)).reshape(X_test0.shape)\n",
    "        y_test = scaler.transform(y_test0.reshape(-1, 1)).reshape(-1)\n",
    "        subsequences = 1\n",
    "        timesteps = X_train.shape[1] // subsequences\n",
    "        X_train = X_train.reshape((X_train.shape[0], subsequences, timesteps, 1))\n",
    "        X_test = X_test.reshape((X_test.shape[0], subsequences, timesteps, 1))\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        # Define the CNN-LSTM model\n",
    "        model = Sequential()\n",
    "\n",
    "        # CNN layers within TimeDistributed wrapper to process each subsequence\n",
    "        model.add(TimeDistributed(Conv1D(filters=64, kernel_size=3, activation='relu'), input_shape=(None, timesteps, 1)))\n",
    "        model.add(TimeDistributed(MaxPooling1D(pool_size=2)))\n",
    "        model.add(TimeDistributed(Flatten()))\n",
    "\n",
    "        # LSTM layers\n",
    "        model.add(LSTM(128, return_sequences=False))\n",
    "        model.add(Dropout(0.2))\n",
    "\n",
    "        # Final dense layers\n",
    "        # model.add(Dense(64, activation='relu'))\n",
    "        model.add(Dense(steps))  # Adjust 'steps' if you're predicting more than one future step\n",
    "\n",
    "        # Compile the model\n",
    "        model.compile(optimizer=Adam(0.001), loss='mean_squared_error')\n",
    "\n",
    "        # Train the model\n",
    "        model.fit(X_train, y_train, epochs=50, batch_size=1, verbose=1, validation_data=(X_train, y_train))\n",
    "\n",
    "        # Predictions\n",
    "        train_predict = model.predict(X_train)\n",
    "        test_predict = model.predict(X_test)\n",
    "\n",
    "        # Inverse transform the predictions back to original scale\n",
    "        train_predict = scaler.inverse_transform(train_predict)\n",
    "        test_predict = scaler.inverse_transform(test_predict)\n",
    "        y_train = scaler.inverse_transform(y_train.reshape(-1,1))\n",
    "        y_test = scaler.inverse_transform(y_test.reshape(-1,1))\n",
    "\n",
    "        def calculate_mape(y_true, y_pred):\n",
    "            return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "        def calculate_directional_accuracy(y_true, y_pred):\n",
    "            correct = np.sum(np.sign(y_true) == np.sign(y_pred))\n",
    "            return correct / len(y_true)\n",
    "\n",
    "\n",
    "\n",
    "        # Evaluate the model using MAE and RMSE\n",
    "        mae_train = mean_absolute_error(y_train.reshape(-1,1), train_predict)\n",
    "        rmse_train = np.sqrt(mean_squared_error(y_train.reshape(-1,1),train_predict))\n",
    "        mape_train = calculate_mape(y_train.reshape(-1,1), train_predict)\n",
    "        direction_train = calculate_directional_accuracy(y_train.reshape(-1,1), train_predict)\n",
    "\n",
    "        mae_test = mean_absolute_error(y_test0.reshape(-1,1), test_predict)\n",
    "        rmse_test = np.sqrt(mean_squared_error(y_test0.reshape(-1,1), test_predict))\n",
    "        mape_test = calculate_mape(y_test0.reshape(-1,1), test_predict)\n",
    "        direction_test = calculate_directional_accuracy(y_test0.reshape(-1,1), test_predict)\n",
    "\n",
    "        print(f\"Training Data - MAE: {mae_train}, RMSE: {rmse_train}, MAPE: {mape_train}%, Directional Accuracy: {direction_train}\")\n",
    "        print(f\"Testing Data - MAE: {mae_test}, RMSE: {rmse_test}, MAPE: {mape_test}%, Directional Accuracy: {direction_test}\")\n",
    "        metrics1 = {\n",
    "                'RMSE': rmse_test,\n",
    "                # 'MSE': mse,\n",
    "                'MAE': mae_test,\n",
    "                'MAPE':mape_test}\n",
    "                # 'Directional Accuracy': direction_accuracy,\n",
    "                # 'actual_price':y_test_actual,\n",
    "                # 'predictions': transformer_predictions\n",
    "            \n",
    "        # print(metrics1)\n",
    "        metrics2={'actual_price':np.squeeze(y_test0),\n",
    "                'predictions': np.squeeze(test_predict)}\n",
    "        # df_metrics = pd.DataFrame([metrics],index=[0])\n",
    "        # metrics.to_excel('model_metrics.xlsx', index=metrics)\n",
    "        df_metrics1 = pd.DataFrame(metrics1,index=[0])\n",
    "        df_metrics2 = pd.DataFrame(metrics2)\n",
    "        df_metrics=pd.concat([df_metrics1,df_metrics2])\n",
    "        # df_metrics.to_excel('model_metrics_random_forest84.xlsx', index=metrics)\n",
    "        df_metrics.to_excel(writer, sheet_name=f'Run_{r}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVR预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVR\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "data_path = 'golddemandQ.csv'  # Update this to the correct path where IBM.csv is located\n",
    "data = pd.read_csv(data_path)\n",
    "data=data.iloc[:,0:2]\n",
    "data.set_index('time', inplace=True)\n",
    "\n",
    "\n",
    "def create_dataset(data, time_step=2):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - time_step):\n",
    "        X.append(data[i:i + time_step])\n",
    "        y.append(data[i + time_step])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Create dataset\n",
    "X, y = create_dataset(data[['closep']].values, time_step)\n",
    "\n",
    "# Split data into training and testing sets (80% for training, 20% for testing)\n",
    "train_size = int(len(X) * 0.8)\n",
    "X_train0, y_train0 = X[:train_size], y[:train_size]\n",
    "X_test0, y_test0 = X[train_size:], y[train_size:]\n",
    "\n",
    "# Standardize the training data\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "X_train= scaler.fit_transform(X_train0.reshape(-1, 1)).reshape(X_train0.shape)\n",
    "y_train = scaler.fit_transform(y_train0.reshape(-1, 1)).reshape(-1)\n",
    "\n",
    "# Apply the same scaling to the test data\n",
    "X_test = scaler.transform(X_test0.reshape(-1, 1)).reshape(X_test0.shape)\n",
    "y_test = scaler.transform(y_test0.reshape(-1, 1)).reshape(-1)\n",
    "\n",
    "\n",
    "\n",
    "# Instantiate and fit the SVR model with initial parameters\n",
    "svr_rbf = SVR(kernel='rbf')\n",
    "\n",
    "# Use GridSearchCV or a similar method to adaptively find better hyperparameters based on initial performance\n",
    "parameters = {'C': [1e-1, 1e0, 1e1, 1e2], 'gamma': [1e-1, 1e-2, 1e-3, 1e-4], 'epsilon': [1e-1, 1e-2, 1e-3, 1e-4]}\n",
    "grid_search = GridSearchCV(svr_rbf, parameters, cv=5)\n",
    "grid_search.fit(np.squeeze(X_train), y_train.ravel())\n",
    "\n",
    "best_svr = grid_search.best_estimator_\n",
    "\n",
    "# Predictions with the optimized model\n",
    "train_predict = best_svr.predict(np.squeeze(X_train))\n",
    "test_predict = best_svr.predict(np.squeeze(X_test))\n",
    "\n",
    "# Inverse transform the predictions back to original scale\n",
    "train_predict = scaler.inverse_transform(train_predict.reshape(-1, 1))\n",
    "test_predict = scaler.inverse_transform(test_predict.reshape(-1, 1))\n",
    "y_train_inv = scaler.inverse_transform(y_train.reshape(-1, 1))\n",
    "y_test_inv = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
    "\n",
    "# Evaluate the model using MAE, RMSE, MAPE, and Directional Accuracy\n",
    "def calculate_mape(y_true, y_pred):\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "def calculate_directional_accuracy(y_true, y_pred):\n",
    "    correct = np.sum(np.sign(y_true) == np.sign(y_pred))\n",
    "    return correct / len(y_true)\n",
    "\n",
    "mae_train = mean_absolute_error(y_train_inv, train_predict)\n",
    "rmse_train = np.sqrt(mean_squared_error(y_train_inv, train_predict))\n",
    "mape_train = calculate_mape(y_train_inv, train_predict)\n",
    "direction_train = calculate_directional_accuracy(y_train_inv, train_predict)\n",
    "\n",
    "mae_test = mean_absolute_error(y_test_inv, test_predict)\n",
    "rmse_test = np.sqrt(mean_squared_error(y_test_inv, test_predict))\n",
    "mape_test = calculate_mape(y_test_inv, test_predict)\n",
    "direction_test = calculate_directional_accuracy(y_test_inv, test_predict)\n",
    "\n",
    "print(f\"Training Data - MAE: {mae_train}, RMSE: {rmse_train}, MAPE: {mape_train}%, Directional Accuracy: {direction_train}\")\n",
    "print(f\"Testing Data - MAE: {mae_test}, RMSE: {rmse_test}, MAPE: {mape_test}%, Directional Accuracy: {direction_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVR  带有数据增强\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from scipy.stats import pearsonr\n",
    "from GANg import GANtest\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "# Set random seed for reproducibility\n",
    "from tensorflow import keras\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "data_path = 'golddemandQ.csv'  # Update this to the correct path where IBM.csv is located\n",
    "data = pd.read_csv(data_path)\n",
    "data=data.iloc[:,0:2]\n",
    "data.set_index('time', inplace=True)\n",
    "\n",
    "\n",
    "def create_dataset(data, time_step=2):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - time_step):\n",
    "        X.append(data[i:i + time_step])\n",
    "        y.append(data[i + time_step])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Create dataset\n",
    "X, y = create_dataset(data[['closep']].values, time_step)\n",
    "\n",
    "# Split data into training and testing sets (80% for training, 20% for testing)\n",
    "train_size = int(len(X) * 0.8)\n",
    "X_train0, y_train0 = X[:train_size], y[:train_size]\n",
    "X_test0, y_test0 = X[train_size:], y[train_size:]\n",
    "\n",
    "# Standardize the training data\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "X_train0= scaler.fit_transform(X_train0.reshape(-1, 1)).reshape(X_train0.shape)\n",
    "y_train0 = scaler.fit_transform(y_train0.reshape(-1, 1)).reshape(-1)\n",
    "\n",
    "# Apply the same scaling to the test data\n",
    "X_test = scaler.transform(X_test0.reshape(-1, 1)).reshape(X_test0.shape)\n",
    "y_test = scaler.transform(y_test0.reshape(-1, 1)).reshape(-1)\n",
    "\n",
    "'''data augmentation'''\n",
    "X_train0=pd.DataFrame(X_train0.reshape(-1,2))\n",
    "y_train0=pd.DataFrame(y_train0.reshape(-1,1))\n",
    "gen=GANtest(X_train0)\n",
    "gen.columns = X_train0.columns\n",
    "\n",
    "\n",
    "'''LRmodel prediction target'''\n",
    "from sklearn.linear_model import LinearRegression\n",
    "best_svr0 = LinearRegression()\n",
    "best_svr0.fit(X_train0, y_train0)\n",
    "\n",
    "\n",
    "# Predictions with the optimized model\n",
    "train_predict0 = best_svr0.predict(gen)\n",
    "# train_predict0.columns = y_train0.columns\n",
    "train_predict0=pd.DataFrame(train_predict0)\n",
    "y_train=pd.concat([y_train0,train_predict0],axis=0)\n",
    "X_train=pd.concat([X_train0,gen],axis=0)\n",
    "\n",
    "\n",
    "# Instantiate and fit the SVR model with initial parameters\n",
    "# svr_rbf = SVR(kernel='rbf', C=100, gamma=0.1, epsilon=.1)\n",
    "# # Use GridSearchCV or a similar method to adaptively find better hyperparameters based on initial performance\n",
    "# parameters = {'C': [1e-1, 1e0, 1e1, 1e2], 'gamma': [1e-1, 1e-2, 1e-3, 1e-4], 'epsilon': [1e-1, 1e-2, 1e-3, 1e-4]}\n",
    "# grid_search = GridSearchCV(svr_rbf, parameters, cv=5)\n",
    "# grid_search.fit(X_train, y_train)\n",
    "# best_svr = grid_search.best_estimator_\n",
    "\n",
    "best_svr=BaggingRegressor(random_state=42)\n",
    "# best_svr=BaggingRegressor(estimator=KNeighborsRegressor(n_neighbors=5), n_estimators=100)\n",
    "best_svr.fit(X_train, y_train)\n",
    "# Predictions with the optimized model\n",
    "train_predict = best_svr.predict(np.squeeze(X_train))\n",
    "test_predict = best_svr.predict(np.squeeze(X_test))\n",
    "\n",
    "# Inverse transform the predictions back to original scale\n",
    "train_predict = scaler.inverse_transform(train_predict.reshape(-1, 1))\n",
    "test_predict = scaler.inverse_transform(test_predict.reshape(-1, 1))\n",
    "y_train_inv = scaler.inverse_transform(np.array(y_train).reshape(-1, 1))\n",
    "y_test_inv = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
    "\n",
    "# Evaluate the model using MAE, RMSE, MAPE, and Directional Accuracy\n",
    "def calculate_mape(y_true, y_pred):\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "def calculate_directional_accuracy(y_true, y_pred):\n",
    "    correct = np.sum(np.sign(y_true) == np.sign(y_pred))\n",
    "    return correct / len(y_true)\n",
    "\n",
    "mae_train = mean_absolute_error(y_train_inv, train_predict)\n",
    "rmse_train = np.sqrt(mean_squared_error(y_train_inv, train_predict))\n",
    "mape_train = calculate_mape(y_train_inv, train_predict)\n",
    "direction_train = calculate_directional_accuracy(y_train_inv, train_predict)\n",
    "\n",
    "mae_test = mean_absolute_error(y_test_inv, test_predict)\n",
    "rmse_test = np.sqrt(mean_squared_error(y_test_inv, test_predict))\n",
    "mape_test = calculate_mape(y_test_inv, test_predict)\n",
    "direction_test = calculate_directional_accuracy(y_test_inv, test_predict)\n",
    "\n",
    "print(f\"Training Data - MAE: {mae_train}, RMSE: {rmse_train}, MAPE: {mape_train}%, Directional Accuracy: {direction_train}\")\n",
    "print(f\"Testing Data - MAE: {mae_test}, RMSE: {rmse_test}, MAPE: {mape_test}%, Directional Accuracy: {direction_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#LSTM + data augmentation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from GANg import GANtest\n",
    "with pd.ExcelWriter('LSTMdataaugmentation-demand-mult_sheets.xlsx') as writer:\n",
    "    for r in range(10):  # \n",
    "        # Set random seed for reproducibility\n",
    "        np.random.seed(42)\n",
    "\n",
    "        # Load the data\n",
    "        data_path = 'golddemandQ.csv'  # Update this to the correct path where IBM.csv is located\n",
    "        data = pd.read_csv(data_path)\n",
    "        data=data.iloc[:,[0,2]]\n",
    "\n",
    "        def create_dataset(data, time_step=2):\n",
    "            X, y = [], []\n",
    "            for i in range(len(data) - time_step):\n",
    "                X.append(data[i:i + time_step])\n",
    "                y.append(data[i + time_step])\n",
    "            return np.array(X), np.array(y)\n",
    "\n",
    "        # Create dataset\n",
    "        X, y = create_dataset(data[['closed']].values, time_step)\n",
    "\n",
    "        # Split data into training and testing sets (80% for training, 20% for testing)\n",
    "        train_size = int(len(X) * 0.8)\n",
    "        X_train0, y_train0 = X[:train_size], y[:train_size]\n",
    "        X_test0, y_test0 = X[train_size:], y[train_size:]\n",
    "\n",
    "        # Standardize the training data\n",
    "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        X_train0= scaler.fit_transform(X_train0.reshape(-1, 1)).reshape(X_train0.shape)\n",
    "        y_train0 = scaler.fit_transform(y_train0.reshape(-1, 1)).reshape(-1)\n",
    "\n",
    "        # Apply the same scaling to the test data\n",
    "        X_test = scaler.transform(X_test0.reshape(-1, 1)).reshape(X_test0.shape)\n",
    "        y_test = scaler.transform(y_test0.reshape(-1, 1)).reshape(-1)\n",
    "\n",
    "\n",
    "        ''' data augmentation'''\n",
    "        X_train0=pd.DataFrame(X_train0.reshape(-1,2))\n",
    "        y_train0=pd.DataFrame(y_train0.reshape(-1,1))\n",
    "        gen=GANtest(X_train0)\n",
    "        gen.columns = X_train0.columns\n",
    "        '''LR'''\n",
    "        from sklearn.linear_model import LinearRegression\n",
    "        best_svr0 = LinearRegression()\n",
    "        best_svr0.fit(X_train0, y_train0)\n",
    " \n",
    "        # Predictions with the optimized model\n",
    "        train_predict0 = best_svr0.predict(gen)\n",
    "        # train_predict0.columns = y_train0.columns\n",
    "        train_predict0=pd.DataFrame(train_predict0)\n",
    "        y_train=pd.concat([y_train0,train_predict0],axis=0)\n",
    "        X_train=pd.concat([X_train0,gen],axis=0)\n",
    "        X_train= np.array(X_train).reshape(X_train.shape[0],X_train.shape[1],1)\n",
    "        y_train =np.array(y_train).reshape(-1)\n",
    "\n",
    "\n",
    "        # Build the LSTM model\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(128, input_shape=(sequence_length, 1)))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(steps))  # The output layer should have 'steps' units if predicting multiple steps\n",
    "\n",
    "        # Compile the model\n",
    "        model.compile(optimizer=Adam(0.001), loss='mean_squared_error')\n",
    "\n",
    "        # Train the model\n",
    "        model.fit(X_train, y_train, epochs=50, batch_size=1, verbose=1, validation_data=(X_test, y_test))\n",
    "\n",
    "        # Predicting\n",
    "        train_predict = model.predict(X_train)\n",
    "        test_predict = model.predict(X_test)\n",
    "\n",
    "        # Inverse transform the predictions back to original scale\n",
    "        train_predict = scaler.inverse_transform(train_predict)\n",
    "        test_predict = scaler.inverse_transform(test_predict)\n",
    "        y_train_inv = scaler.inverse_transform(y_train.reshape(-1, 1))\n",
    "        y_test_inv = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
    "\n",
    "        # Evaluate the model using MAE, RMSE, MAPE, and Directional Accuracy\n",
    "        def calculate_mape(y_true, y_pred):\n",
    "            return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "        def calculate_directional_accuracy(y_true, y_pred):\n",
    "            correct = np.sum(np.sign(y_true) == np.sign(y_pred))\n",
    "            return correct / len(y_true)\n",
    "\n",
    "        mae_train = mean_absolute_error(y_train_inv, train_predict)\n",
    "        rmse_train = np.sqrt(mean_squared_error(y_train_inv, train_predict))\n",
    "        mape_train = calculate_mape(y_train_inv, train_predict)\n",
    "        direction_train = calculate_directional_accuracy(y_train_inv, train_predict)\n",
    "\n",
    "        mae_test = mean_absolute_error(y_test_inv, test_predict)\n",
    "        rmse_test = np.sqrt(mean_squared_error(y_test_inv, test_predict))\n",
    "        mape_test = calculate_mape(y_test_inv, test_predict)\n",
    "        direction_test = calculate_directional_accuracy(y_test_inv, test_predict)\n",
    "\n",
    "        print(f\"Training Data - MAE: {mae_train}, RMSE: {rmse_train}, MAPE: {mape_train}%, Directional Accuracy: {direction_train}\")\n",
    "        print(f\"Testing Data - MAE: {mae_test}, RMSE: {rmse_test}, MAPE: {mape_test}%, Directional Accuracy: {direction_test}\")\n",
    "\n",
    "        metrics1 = {\n",
    "                'RMSE': rmse_test,\n",
    "                # 'MSE': mse,\n",
    "                'MAE': mae_test,\n",
    "                'MAPE':mape_test,\n",
    "                # 'Directional Accuracy': direction_accuracy,\n",
    "                # 'actual_price':y_test_actual,\n",
    "                # 'predictions': transformer_predictions\n",
    "            }\n",
    "        # print(metrics1)\n",
    "        metrics2={'actual_price':np.squeeze(y_test_inv),\n",
    "                'predictions': np.squeeze(test_predict)}\n",
    "        \n",
    "        # df_metrics = pd.DataFrame([metrics],index=[0])\n",
    "        # metrics.to_excel('model_metrics.xlsx', index=metrics)\n",
    "        df_metrics1 = pd.DataFrame(metrics1,index=[0])\n",
    "        df_metrics2 = pd.DataFrame(metrics2)\n",
    "        df_metrics=pd.concat([df_metrics1,df_metrics2])\n",
    "        # df_metrics.to_excel('model_metrics_random_forest84.xlsx', index=metrics)\n",
    "        df_metrics.to_excel(writer, sheet_name=f'Run_{r}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!pip install prophet\n",
    "!pip install pystan = 2.19.1.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prophet import Prophet\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, median_absolute_error, mean_squared_log_error\n",
    "\n",
    "# Load the data\n",
    "data_path = 'goldpd.xlsx'  # Update this to the correct path where IBM.csv is located\n",
    "data = pd.read_excel(data_path)\n",
    "data=data.iloc[:,0:2]\n",
    "data['time'] = pd.to_datetime(data['time']).dt.date\n",
    "valid=data[int(0.8*len(data)):]\n",
    "train=data[:int(0.8*len(data))]\n",
    "train_prophet = pd.DataFrame()\n",
    "train_prophet['ds'] = train.time\n",
    "train_prophet['y'] = train.closep\n",
    "\n",
    "\n",
    "#instantiate Prophet with only yearly seasonality as our data is monthly \n",
    "# model = Prophet( yearly_seasonality=True)\n",
    "model = Prophet()\n",
    "model.fit(train_prophet) #fit the model with your dataframe\n",
    "# predict for five months in the furure and MS - month start is the frequency\n",
    "future = model.make_future_dataframe(periods = len(valid), freq = 'Q') \n",
    "future.tail()\n",
    "\n",
    "\n",
    "# now lets make the forecasts\n",
    "forecast = model.predict(future)\n",
    "forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail()\n",
    "\n",
    "fig = model.plot(forecast)\n",
    "#plot the predictions for validation set\n",
    "plt.plot(valid['time'],valid['closep'], label='Valid', color = 'red', linewidth = 2)\n",
    "plt.show()\n",
    "\n",
    "y_prophet = pd.DataFrame()\n",
    "y_prophet['ds'] = valid.time\n",
    "y_prophet['y'] = valid.closep\n",
    "\n",
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "def evaluate_forecast(y,pred):\n",
    "    results = pd.DataFrame({'r2_score':r2_score(y, pred),\n",
    "                           }, index=[0])\n",
    "    results['mean_absolute_error'] = mean_absolute_error(y, pred)\n",
    "    results['median_absolute_error'] = median_absolute_error(y, pred)\n",
    "    results['mse'] = mean_squared_error(y, pred)\n",
    "    results['msle'] = mean_squared_log_error(y, pred)\n",
    "    results['mape'] = mean_absolute_percentage_error(y, pred)\n",
    "    results['rmse'] = np.sqrt(results['mse'])\n",
    "    return results\n",
    "\n",
    "y_prophet = y_prophet.set_index('ds')\n",
    "forecast_prophet = forecast.set_index('ds')\n",
    "\n",
    "evaluate_forecast(y_prophet.y, forecast_prophet.yhat[int(0.8*len(data)):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVR  month data forecasting 3 steps=quarterly\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "data_path = 'goldm.csv'  # Update this to the correct path where IBM.csv is located\n",
    "data = pd.read_csv(data_path)\n",
    "data=data.iloc[:,0:2]\n",
    "data.set_index('time', inplace=True)\n",
    "\n",
    "# Assuming 'Close' is the column you want to predict\n",
    "# data = data['closep'].values.reshape(-1, 1)\n",
    "\n",
    "# # Normalize the data\n",
    "# scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "# normalized_data = scaler.fit_transform(data)\n",
    "\n",
    "# # Data Preprocessing Function modified for multi-step\n",
    "# def create_sequences(data, sequence_length, steps):\n",
    "#     xs, ys = [], []\n",
    "#     for i in range(len(data) - sequence_length - steps + 1):\n",
    "#         x = data[i:(i + sequence_length)]\n",
    "#         y = data[i + sequence_length:i + sequence_length + steps]\n",
    "#         xs.append(x)\n",
    "#         ys.append(y)\n",
    "#     return np.array(xs), np.array(ys)\n",
    "\n",
    "# # Create sequences with modified function\n",
    "# sequence_length = 2\n",
    "# steps = 1  # Number of future steps to predict\n",
    "# X, y = create_sequences(normalized_data, sequence_length, steps)\n",
    "\n",
    "# # Split the data\n",
    "# split = int(len(X) * 0.8)\n",
    "# X_train, X_test = X[:split], X[split:]\n",
    "# y_train, y_test = y[:split], y[split:]\n",
    "time_step=2\n",
    "ste=3\n",
    "def create_dataset(data, time_step=2, ste=2):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - time_step-ste+1):\n",
    "        X.append(data[i:i + time_step])\n",
    "        y.append(data[i + time_step+ste-1])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Create dataset\n",
    "X, y = create_dataset(data[['close']].values, time_step)\n",
    "\n",
    "# Split data into training and testing sets (80% for training, 20% for testing)\n",
    "train_size = int(len(X) * 0.94)\n",
    "X_train0, y_train0 = X[:train_size], y[:train_size]\n",
    "X_test0, y_test0 = X[train_size:], y[train_size:]\n",
    "\n",
    "# Standardize the training data\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "X_train= scaler.fit_transform(X_train0.reshape(-1, 1)).reshape(X_train0.shape)\n",
    "y_train = scaler.fit_transform(y_train0.reshape(-1, 1)).reshape(-1)\n",
    "\n",
    "# Apply the same scaling to the test data\n",
    "X_test = scaler.transform(X_test0.reshape(-1, 1)).reshape(X_test0.shape)\n",
    "y_test = scaler.transform(y_test0.reshape(-1, 1)).reshape(-1)\n",
    "\n",
    "\n",
    "\n",
    "# Instantiate and fit the SVR model with initial parameters\n",
    "# svr_rbf = SVR(kernel='rbf')\n",
    "# # Use GridSearchCV or a similar method to adaptively find better hyperparameters based on initial performance\n",
    "# parameters = {'C': [1e-1, 1e0, 1e1, 1e2], 'gamma': [1e-1, 1e-2, 1e-3, 1e-4], 'epsilon': [1e-1, 1e-2, 1e-3, 1e-4]}\n",
    "# grid_search = GridSearchCV(svr_rbf, parameters, cv=5)\n",
    "# grid_search.fit(np.squeeze(X_train), y_train.ravel())\n",
    "# best_svr = grid_search.best_estimator_\n",
    "\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "# best_svr=BaggingRegressor()\n",
    "# best_svr=BaggingRegressor(random_state=42)\n",
    "best_svr=BaggingRegressor(estimator=KNeighborsRegressor(n_neighbors=5), n_estimators=100)\n",
    "best_svr.fit(np.squeeze(X_train), y_train.ravel())\n",
    "\n",
    "\n",
    "\n",
    "# Predictions with the optimized model\n",
    "train_predict = best_svr.predict(np.squeeze(X_train))\n",
    "test_predict = best_svr.predict(np.squeeze(X_test))\n",
    "\n",
    "# Inverse transform the predictions back to original scale\n",
    "train_predict = scaler.inverse_transform(train_predict.reshape(-1, 1))\n",
    "test_predict = scaler.inverse_transform(test_predict.reshape(-1, 1))\n",
    "y_train_inv = scaler.inverse_transform(y_train.reshape(-1, 1))\n",
    "y_test_inv = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
    "\n",
    "# Evaluate the model using MAE, RMSE, MAPE, and Directional Accuracy\n",
    "def calculate_mape(y_true, y_pred):\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "def calculate_directional_accuracy(y_true, y_pred):\n",
    "    correct = np.sum(np.sign(y_true) == np.sign(y_pred))\n",
    "    return correct / len(y_true)\n",
    "\n",
    "mae_train = mean_absolute_error(y_train_inv, train_predict)\n",
    "rmse_train = np.sqrt(mean_squared_error(y_train_inv, train_predict))\n",
    "mape_train = calculate_mape(y_train_inv, train_predict)\n",
    "direction_train = calculate_directional_accuracy(y_train_inv, train_predict)\n",
    "\n",
    "mae_test = mean_absolute_error(y_test_inv, test_predict)\n",
    "rmse_test = np.sqrt(mean_squared_error(y_test_inv, test_predict))\n",
    "mape_test = calculate_mape(y_test_inv, test_predict)\n",
    "direction_test = calculate_directional_accuracy(y_test_inv, test_predict)\n",
    "\n",
    "print(f\"Training Data - MAE: {mae_train}, RMSE: {rmse_train}, MAPE: {mape_train}%, Directional Accuracy: {direction_train}\")\n",
    "print(f\"Testing Data - MAE: {mae_test}, RMSE: {rmse_test}, MAPE: {mape_test}%, Directional Accuracy: {direction_test}\")\n",
    "\n",
    "\n",
    "#high frequency tranform low frequency\n",
    "summed_data_true= y_test_inv.reshape(-1, 3).mean(axis=1)\n",
    "summed_data_predict=test_predict.reshape(-1, 3).mean(axis=1)\n",
    "pd.concat([pd.DataFrame(summed_data_true),pd.DataFrame(summed_data_predict)],axis=1).to_csv('yuedujieguo1.csv')\n",
    "mae_te = mean_absolute_error(summed_data_true, summed_data_predict)\n",
    "rmse_te = np.sqrt(mean_squared_error(summed_data_true, summed_data_predict))\n",
    "mape_te = calculate_mape(summed_data_true, summed_data_predict)\n",
    "print(f\"final Testing Data - MAE: {mae_te}, RMSE: {rmse_te}, MAPE: {mape_te}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#LSTM + month data forecasting\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from GANg import GANtest\n",
    "with pd.ExcelWriter('LSTMhighfrequency-price-mult_sheets.xlsx') as writer:\n",
    "    for r in range(10):  \n",
    "        # Set random seed for reproducibility\n",
    "        np.random.seed(42)\n",
    "\n",
    "        # Load the data\n",
    "        data_path = 'goldm.csv'  # Update this to the correct path where IBM.csv is located\n",
    "        data = pd.read_csv(data_path)\n",
    "        data=data.iloc[:,0:2]\n",
    "        data.set_index('time', inplace=True)\n",
    "\n",
    "  \n",
    "        sequence_length = 2  # Updated sequence length for your use case\n",
    "        steps = 1  # Number of future steps to predict\n",
    " \n",
    "\n",
    "        time_step=2\n",
    "        ste=3\n",
    "        def create_dataset(data, time_step=2, ste=2):\n",
    "            X, y = [], []\n",
    "            for i in range(len(data) - time_step-ste+1):\n",
    "                X.append(data[i:i + time_step])\n",
    "                y.append(data[i + time_step+ste-1])\n",
    "            return np.array(X), np.array(y)\n",
    "\n",
    "        # Create dataset\n",
    "        X, y = create_dataset(data[['close']].values, time_step)\n",
    "\n",
    "        # Split data into training and testing sets (80% for training, 20% for testing)\n",
    "        train_size = int(len(X) * 0.94)\n",
    "        X_train0, y_train0 = X[:train_size], y[:train_size]\n",
    "        X_test0, y_test0 = X[train_size:], y[train_size:]\n",
    "\n",
    "        # Standardize the training data\n",
    "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        X_train0= scaler.fit_transform(X_train0.reshape(-1, 1)).reshape(X_train0.shape)\n",
    "        y_train0 = scaler.fit_transform(y_train0.reshape(-1, 1)).reshape(-1)\n",
    "\n",
    "        # Apply the same scaling to the test data\n",
    "        X_test = scaler.transform(X_test0.reshape(-1, 1)).reshape(X_test0.shape)\n",
    "        y_test = scaler.transform(y_test0.reshape(-1, 1)).reshape(-1)\n",
    "\n",
    "\n",
    "        # Build the LSTM model\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(128, input_shape=(sequence_length, 1)))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(steps))  # The output layer should have 'steps' units if predicting multiple steps\n",
    "\n",
    "        # Compile the model\n",
    "        model.compile(optimizer=Adam(0.001), loss='mean_squared_error')\n",
    "\n",
    "        # Train the model\n",
    "        model.fit(X_train, y_train, epochs=50, batch_size=1, verbose=1, validation_data=(X_test, y_test))\n",
    "\n",
    "        # Predicting\n",
    "        train_predict = model.predict(X_train)\n",
    "        test_predict = model.predict(X_test)\n",
    "\n",
    "        # Inverse transform the predictions back to original scale\n",
    "        train_predict = scaler.inverse_transform(train_predict)\n",
    "        test_predict = scaler.inverse_transform(test_predict)\n",
    "        y_train_inv = scaler.inverse_transform(y_train.reshape(-1, 1))\n",
    "        y_test_inv = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
    "\n",
    "        # Evaluate the model using MAE, RMSE, MAPE, and Directional Accuracy\n",
    "        def calculate_mape(y_true, y_pred):\n",
    "            return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "        def calculate_directional_accuracy(y_true, y_pred):\n",
    "            correct = np.sum(np.sign(y_true) == np.sign(y_pred))\n",
    "            return correct / len(y_true)\n",
    "\n",
    "        mae_train = mean_absolute_error(y_train_inv, train_predict)\n",
    "        rmse_train = np.sqrt(mean_squared_error(y_train_inv, train_predict))\n",
    "        mape_train = calculate_mape(y_train_inv, train_predict)\n",
    "        direction_train = calculate_directional_accuracy(y_train_inv, train_predict)\n",
    "\n",
    "        mae_test = mean_absolute_error(y_test_inv, test_predict)\n",
    "        rmse_test = np.sqrt(mean_squared_error(y_test_inv, test_predict))\n",
    "        mape_test = calculate_mape(y_test_inv, test_predict)\n",
    "        direction_test = calculate_directional_accuracy(y_test_inv, test_predict)\n",
    "\n",
    "        print(f\"Training Data - MAE: {mae_train}, RMSE: {rmse_train}, MAPE: {mape_train}%, Directional Accuracy: {direction_train}\")\n",
    "        print(f\"Testing Data - MAE: {mae_test}, RMSE: {rmse_test}, MAPE: {mape_test}%, Directional Accuracy: {direction_test}\")\n",
    "\n",
    "\n",
    "\n",
    "        summed_data_true= y_test_inv.reshape(-1, 3).mean(axis=1)\n",
    "        summed_data_predict=test_predict.reshape(-1, 3).mean(axis=1)\n",
    "        # pd.concat([pd.DataFrame(summed_data_true),pd.DataFrame(summed_data_predict)],axis=1).to_csv('yuedujieguo1.csv')\n",
    "        mae_te = mean_absolute_error(summed_data_true, summed_data_predict)\n",
    "        rmse_te = np.sqrt(mean_squared_error(summed_data_true, summed_data_predict))\n",
    "        mape_te = calculate_mape(summed_data_true, summed_data_predict)\n",
    "        print(f\"final Testing Data - MAE: {mae_te}, RMSE: {rmse_te}, MAPE: {mape_te}%\")\n",
    "\n",
    "\n",
    "        metrics1 = {\n",
    "                'RMSE': rmse_te,\n",
    "                # 'MSE': mse,\n",
    "                'MAE': mae_te,\n",
    "                'MAPE':mape_te,\n",
    "                # 'Directional Accuracy': direction_accuracy,\n",
    "                # 'actual_price':y_test_actual,\n",
    "                # 'predictions': transformer_predictions\n",
    "            }\n",
    "        # print(metrics1)\n",
    "        metrics2={'actual_price':np.squeeze(summed_data_true),\n",
    "                'predictions': np.squeeze(summed_data_predict)}\n",
    "   \n",
    "        # df_metrics = pd.DataFrame([metrics],index=[0])\n",
    "        # metrics.to_excel('model_metrics.xlsx', index=metrics)\n",
    "        df_metrics1 = pd.DataFrame(metrics1,index=[0])\n",
    "        df_metrics2 = pd.DataFrame(metrics2)\n",
    "        df_metrics=pd.concat([df_metrics1,df_metrics2])\n",
    "        # df_metrics.to_excel('model_metrics_random_forest84.xlsx', index=metrics)\n",
    "        df_metrics.to_excel(writer, sheet_name=f'Run_{r}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "multivariables forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVR\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# data_path = 'goldmultivariables.xlsx'  # Update this to the correct path where IBM.csv is located\n",
    "data_path = 'golddemanmultivariables.xlsx'  # Update this to the correct path where IBM.csv is located\n",
    "data = pd.read_excel(data_path)\n",
    "data=data.iloc[:,:]\n",
    "data.set_index('time', inplace=True)\n",
    "\n",
    "train_size = int(len(data) * 0.8)\n",
    "X_train, y_train = data.iloc[:train_size,1:], data.iloc[:train_size,0]\n",
    "X_test, y_test = data.iloc[train_size:,1:], data.iloc[train_size:,0]\n",
    "\n",
    "# Standardize the training data\n",
    "scaler_x = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler_y = MinMaxScaler()\n",
    "\n",
    "\n",
    "X_train = scaler_x.fit_transform(X_train)\n",
    "X_test= scaler_x.transform(X_test)\n",
    "\n",
    "\n",
    "# scalers = {}\n",
    "# for i in range(X_train.shape[1]):\n",
    "#     scalers[i] = MinMaxScaler(feature_range=(0, 1))\n",
    "#     X_train.iloc[:, i] = scalers[i].fit_transform(X_train.iloc[:, i].values.reshape(-1, 1))\n",
    "# for i in range(X_test.shape[1]):\n",
    "#     X_test.iloc[:, i] = scalers[i].transform(X_test.iloc[:, i].values.reshape(-1, 1))\n",
    "\n",
    "y_train = scaler_y.fit_transform(np.array(y_train).reshape(-1, 1)).flatten()\n",
    "y_test = scaler_y.transform(np.array(y_test).reshape(-1, 1)).flatten()\n",
    "\n",
    "# Instantiate and fit the SVR model with initial parameters\n",
    "# svr_rbf = SVR(kernel='rbf')\n",
    "# # Use GridSearchCV or a similar method to adaptively find better hyperparameters based on initial performance\n",
    "# parameters = {'C': [1e-1, 1e0, 1e1, 1e2], 'gamma': [1e-1, 1e-2, 1e-3, 1e-4], 'epsilon': [1e-1, 1e-2, 1e-3, 1e-4]}\n",
    "# grid_search = GridSearchCV(svr_rbf, parameters, cv=5)\n",
    "# grid_search.fit(np.squeeze(X_train), y_train.ravel())\n",
    "# best_svr = grid_search.best_estimator_\n",
    "\n",
    "\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "# best_svr=BaggingRegressor()\n",
    "# best_svr=BaggingRegressor(random_state=42)\n",
    "best_svr=BaggingRegressor(estimator=KNeighborsRegressor(n_neighbors=5), n_estimators=100)\n",
    "best_svr.fit(np.squeeze(X_train), y_train.ravel())\n",
    "\n",
    "\n",
    "# Predictions with the optimized model\n",
    "train_predict = best_svr.predict(np.squeeze(X_train))\n",
    "test_predict = best_svr.predict(np.squeeze(X_test))\n",
    "\n",
    "# Inverse transform the predictions back to original scale\n",
    "# train_predict = scaler.inverse_transform(train_predict.reshape(-1, 1))\n",
    "# test_predict = scaler.inverse_transform(test_predict.reshape(-1, 1))\n",
    "# y_train_inv = scaler.inverse_transform(y_train.reshape(-1, 1))\n",
    "# y_test_inv = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
    "y_test_inv = scaler_y.inverse_transform(y_test.reshape(-1, 1)).flatten()\n",
    "test_predict = scaler_y.inverse_transform(test_predict.reshape(-1, 1)).flatten()\n",
    "# Evaluate the model using MAE, RMSE, MAPE, and Directional Accuracy\n",
    "def calculate_mape(y_true, y_pred):\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "def calculate_directional_accuracy(y_true, y_pred):\n",
    "    correct = np.sum(np.sign(y_true) == np.sign(y_pred))\n",
    "    return correct / len(y_true)\n",
    "\n",
    "\n",
    "\n",
    "mae_test = mean_absolute_error(y_test_inv, test_predict)\n",
    "rmse_test = np.sqrt(mean_squared_error(y_test_inv, test_predict))\n",
    "mape_test = calculate_mape(y_test_inv, test_predict)\n",
    "direction_test = calculate_directional_accuracy(y_test_inv, test_predict)\n",
    "\n",
    "\n",
    "print(f\"Testing Data - MAE: {mae_test}, RMSE: {rmse_test}, MAPE: {mape_test}%, Directional Accuracy: {direction_test}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lowtsdf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
